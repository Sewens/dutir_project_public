{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 只更新了pre word count这个类 更新时候仅需要更新这个即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T02:09:25.440498Z",
     "start_time": "2019-07-25T02:09:25.428983Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#导入PCA算法库\n",
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence\n",
    "from sklearn import ensemble\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn.externals import joblib\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T02:09:26.273287Z",
     "start_time": "2019-07-25T02:09:26.156980Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "class esPaperRetrieval():\n",
    "    '''\n",
    "    根据论文检索需求进行功能的微调\n",
    "    '''\n",
    "    _instance = None\n",
    "    _first_init = True\n",
    "\n",
    "    def __new__(cls, *args, **kw):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(esPaperRetrieval, cls).__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self, host, port):\n",
    "        '''\n",
    "        使用ES进行论文检索 指定host、port以及专利index之后进行检索\n",
    "        '''\n",
    "        super(esPaperRetrieval, self).__init__()\n",
    "        self.es = Elasticsearch(hosts=host, port=port, timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "        self.indexName = 'paper-detail-index'\n",
    "\n",
    "    def do_search(self, titleQuery, kwordQuery, summaryQuery, pubQuery, fromDate, toDate, volume):\n",
    "        '''\n",
    "        do_search方法执行具体检索过程\n",
    "        titleQuery 为对应标题检索词\n",
    "        kwordQuery 为对应关键字检索词\n",
    "        summaryQuery 为对应摘要检索词\n",
    "        上述三个词为or模式 可以出现 可以不出现\n",
    "        pubQuery 为对应文摘关键词 这个关键词必定匹配\n",
    "        fromDate 为检索字段起始日期 toDate为终结日期 日期格式 yyyy-mm-dd\n",
    "        volume为每次检索返回的数目\n",
    "        '''\n",
    "        queryBody = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"should\": [\n",
    "                        {\n",
    "                            \"term\": {\n",
    "                                \"P_Title\": titleQuery\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"term\": {\n",
    "                                \"P_Keyword\": kwordQuery\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"term\": {\n",
    "                                \"P_Summary\": summaryQuery\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"term\": {\n",
    "                                \"P_Publication.keyword\": {\"value\": pubQuery}\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \"filter\": [\n",
    "                        {\n",
    "                            \"range\": {\n",
    "                                \"P_year\": {\n",
    "                                    \"gt\": fromDate,\n",
    "                                    \"lt\": toDate\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"from\": 0,\n",
    "            \"size\": volume,\n",
    "            \"sort\": [],\n",
    "            \"aggs\": {}\n",
    "        }\n",
    "        #         print(queryBody)\n",
    "        result = self.es.search(index=self.indexName, body=queryBody)\n",
    "        return result\n",
    "\n",
    "    def format_search(self, result):\n",
    "        '''\n",
    "        format_search方法对检索结果进行格式化 构建符合要求的字段进行返回\n",
    "        输入result为检索结果 提取其中的检索结果进行后处理\n",
    "        使用ES检索后得到的结果中result['hits']['hits']为数组格式数据\n",
    "        其中每一个元素为一个dict 对应部分字段\n",
    "        '''\n",
    "        docs = result['hits']['hits']\n",
    "        docs = [i['_source'] for i in docs]\n",
    "        targetKeyList = 'P_ID, P_Title, P_Author, P_Publication, P_Organ, P_year, P_Keyword, \\\n",
    "    P_Summary, P_Keyword_seg, P_Title_seg,\\\n",
    "    P_Summary_seg, P_URL, P_Fields, P_Fields_two,P_References, P_Pagecount, P_Page, P_Language,\\\n",
    "    P_Download_num, P_Citation_num,P_Vector,P_Volume, P_Issue,P_Issn,P_Isbn, P_Doi,\\\n",
    "    P_Red1, P_Red2, P_Red3, P_Red4, P_Red5'\n",
    "        targetKeyList = [i.strip() for i in targetKeyList.split(',')]\n",
    "        dict_filter_by_keys = lambda d: {k: d[k] for k in targetKeyList}\n",
    "        dict_filter_text = lambda d: {k if not k == 'text' else 'claim_text': d[k] for k in d}\n",
    "        dict_filter_id = lambda d: {k if not k == '_id' else 'id': d[k] for k in d}\n",
    "        docs = (dict_filter_by_keys(doc) for doc in docs)\n",
    "        docs = [dict_filter_text(doc) for doc in docs]\n",
    "        return docs\n",
    "\n",
    "    def Retrieval(self, titleQuery, kwordQuery, summaryQuery, pubQuery, fromDate, toDate, volume):\n",
    "        result = self.do_search(titleQuery, kwordQuery, summaryQuery, pubQuery, fromDate, toDate, volume)\n",
    "        docs = self.format_search(result)\n",
    "        return docs\n",
    "\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T02:09:27.833391Z",
     "start_time": "2019-07-25T02:09:27.616075Z"
    }
   },
   "outputs": [],
   "source": [
    "class author_search():\n",
    "    '''\n",
    "    根据论文检索需求进行功能的微调\n",
    "    '''\n",
    "    _instance = None\n",
    "    _first_init = True\n",
    "\n",
    "    def __new__(cls, *args, **kw):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(author_search, cls).__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self, host, port):\n",
    "        '''\n",
    "        使用ES进行论文检索 指定host、port以及专利index之后进行检索\n",
    "        '''\n",
    "        super(author_search, self).__init__()\n",
    "        self.es = Elasticsearch(hosts=host, port=port, timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "        self.indexName = 'paper-detail-index'\n",
    "\n",
    "    def do_search(self, nameQuery, organQuery, volume):\n",
    "        '''\n",
    "        do_search方法执行具体检索过程\n",
    "        titleQuery 为对应标题检索词\n",
    "        kwordQuery 为对应关键字检索词\n",
    "        summaryQuery 为对应摘要检索词\n",
    "        上述三个词为or模式 可以出现 可以不出现\n",
    "        pubQuery 为对应文摘关键词 这个关键词必定匹配\n",
    "        fromDate 为检索字段起始日期 toDate为终结日期 日期格式 yyyy-mm-dd\n",
    "        volume为每次检索返回的数目\n",
    "        '''\n",
    "        queryBody = {\n",
    "            \"query\": {\n",
    "                \"match_phrase\": {\n",
    "                    \"P_Author\": {\n",
    "                        \"query\": nameQuery,\n",
    "                        \"slop\": 2\n",
    "                    }\n",
    "                }},\n",
    "            \"from\": 0,\n",
    "            \"sort\": [],\n",
    "            \"size\": volume,\n",
    "            \"aggs\": {}\n",
    "        }\n",
    "        self.name = nameQuery\n",
    "        self.organ = organQuery\n",
    "        result = self.es.search(index=self.indexName, body=queryBody)\n",
    "        return result\n",
    "\n",
    "    def format_search(self, result):\n",
    "        '''\n",
    "        format_search方法对检索结果进行格式化 构建符合要求的字段进行返回\n",
    "        输入result为检索结果 提取其中的检索结果进行后处理\n",
    "        使用ES检索后得到的结果中result['hits']['hits']为数组格式数据\n",
    "        其中每一个元素为一个dict 对应部分字段\n",
    "        '''\n",
    "        docs = result['hits']['hits']\n",
    "        docs = [i['_source'] for i in docs]\n",
    "        targetKeyList = 'P_Author,P_Title,  P_Publication, P_Organ, P_Keyword, P_Title_seg,P_Red3,P_Keyword,P_Red1,P_Red2'\n",
    "        targetKeyList = [i.strip() for i in targetKeyList.split(',')]\n",
    "        dict_filter_by_keys = lambda d: {k: d[k] for k in targetKeyList}\n",
    "        dict_filter_text = lambda d: {k if not k == 'text' else 'claim_text': d[k] for k in d}\n",
    "        dict_filter_id = lambda d: {k if not k == '_id' else 'id': d[k] for k in d}\n",
    "        docs = (dict_filter_by_keys(doc) for doc in docs)\n",
    "        docs = [dict_filter_text(doc) for doc in docs]\n",
    "        return docs\n",
    "\n",
    "    def Retrieval(self, nameQuery, organQuery, volume):\n",
    "        result = self.do_search(nameQuery, organQuery, volume)\n",
    "        self.docs = self.format_search(result)\n",
    "        return self.result()\n",
    "\n",
    "    def result(self, ):\n",
    "        title = []\n",
    "        first_authors = []\n",
    "        authors = []\n",
    "        parter_authors = []\n",
    "        organ = []\n",
    "        keywords = []\n",
    "        jounal = []\n",
    "        first_organ = []\n",
    "        for i in range(len(self.docs)):\n",
    "            title.append(self.docs[i]['P_Title'])\n",
    "            authors.append(self.docs[i]['P_Author'])\n",
    "            first_authors.append(authors[i].split(';')[0])\n",
    "            parter_authors.append(authors[i].split(';')[1:])\n",
    "            organ.append(self.docs[i]['P_Organ'])\n",
    "            keywords.append(self.docs[i]['P_Keyword'])\n",
    "            jounal.append(self.docs[i]['P_Publication'])\n",
    "            first_organ.append(organ[i].split(';')[0])\n",
    "            df = pd.DataFrame({\n",
    "                'title': title,\n",
    "                'first_authors': first_authors,\n",
    "                'authors': authors,\n",
    "                'parter_authors': parter_authors,\n",
    "                'organ': organ,\n",
    "                'keywords': keywords,\n",
    "                'Journal': jounal,\n",
    "                'first_authors_organ': first_organ\n",
    "            })\n",
    "        self.papers_author = df.drop_duplicates(subset=['title'], keep='first')\n",
    "        return self.search()\n",
    "\n",
    "    def savepickle(self, name, data):\n",
    "        output = open('%s.pkl' % (name), 'wb')\n",
    "        pickle.dump(data, output)\n",
    "        output.close()\n",
    "\n",
    "    def loadpickle(self, name):\n",
    "        pkl_file = open('%s.pkl' % (name), 'rb')\n",
    "        data1 = pickle.load(pkl_file)\n",
    "        return data1\n",
    "\n",
    "    def commonElement(self, a, b):\n",
    "        commonEle = [val for val in a if val in b]\n",
    "        return commonEle\n",
    "\n",
    "    def all_list(self, list1):\n",
    "        result = {}\n",
    "        for i in set(list1):\n",
    "            result[i] = list1.count(i)\n",
    "        return result\n",
    "\n",
    "    def search(self, ):\n",
    "        self.authors_dict = {}\n",
    "        self.author_dict = {}\n",
    "        quchong_list = []\n",
    "        quchong_list2 = []\n",
    "        self.author_index = 0\n",
    "        j = 0\n",
    "        aa = \"没有找到相匹配的学者\"\n",
    "        self.author_list = list(self.papers_author['first_authors'])\n",
    "        author_organ_list = list(self.papers_author['first_authors_organ'])\n",
    "        self.index_list = [idx for idx, i in enumerate(self.author_list) if i == self.name]\n",
    "        if len(self.index_list) == 0:\n",
    "            print(\"没有相匹配的作者信息\")\n",
    "            return aa\n",
    "        for i in self.index_list:\n",
    "            mm = self.author_list[i] + ' 机构:' + author_organ_list[i]\n",
    "            if author_organ_list[i][0:3] not in quchong_list:\n",
    "                self.authors_dict[j] = mm\n",
    "                j += 1\n",
    "            quchong_list.append(author_organ_list[i][0:3])\n",
    "\n",
    "        for i in self.index_list:\n",
    "            mm = self.author_list[i] + ' 机构:' + author_organ_list[i]\n",
    "            if author_organ_list[i][0:3] not in quchong_list2:\n",
    "                self.author_dict[j] = mm\n",
    "                if self.organ in mm:\n",
    "                    self.author_index = j\n",
    "                    break\n",
    "                else:\n",
    "                    j += 1\n",
    "            quchong_list2.append(author_organ_list[i][0:3])\n",
    "        return self.author()\n",
    "\n",
    "    def author(self):\n",
    "        if self.organ == '':\n",
    "            print(self.authors_dict)\n",
    "            print(\"请输入作者的所属机构\")\n",
    "            return \"qqqq\"\n",
    "        n = int(self.author_index)\n",
    "        haha = str(self.author_dict[n]).split(' ')\n",
    "        name = str(haha[0])\n",
    "        ha = str(haha[1])\n",
    "        organ = str(ha.split(':')[1])\n",
    "        paper = []\n",
    "        parter = []\n",
    "        jounal = []\n",
    "        keywords = []\n",
    "        paper_list = list(self.papers_author['title'])\n",
    "        author_list = list(self.papers_author['first_authors'])\n",
    "        authors_list = list(self.papers_author['authors'])\n",
    "        parter_list = list(self.papers_author['parter_authors'])\n",
    "        authors_organ_list = list(self.papers_author['organ'])\n",
    "        keywords_list = list(self.papers_author['keywords'])\n",
    "        jounal_list = list(self.papers_author['Journal'])\n",
    "        author_index = [idx for idx, author in enumerate(authors_list) if name in author.split(';')]\n",
    "        organ_index = [idx for idx, organs in enumerate(authors_organ_list) if organ in organs.split(';')]\n",
    "        res_list = self.commonElement(author_index, organ_index)\n",
    "        for j in res_list:\n",
    "            a = paper_list[j]\n",
    "            e = jounal_list[j]\n",
    "            jounal.append(e)\n",
    "            paper.append(a)\n",
    "        tonghang = []\n",
    "        qikan = []\n",
    "        for j in res_list:\n",
    "            e = jounal_list[j]\n",
    "            if e not in qikan:\n",
    "                qikan.append(e)\n",
    "\n",
    "        for j in res_list:\n",
    "            d = re.split(';;|;', keywords_list[j])\n",
    "            for ii in d:\n",
    "                if ii != '' and len(ii) < 10:\n",
    "                    keywords.append(ii)\n",
    "        # 同行、合作者\n",
    "        for j in res_list:\n",
    "            b = str(parter_list[j]).split(\" \")\n",
    "            for ii in b:\n",
    "                if ii != 'none' and ii != name and ii != '' and ii != ']':\n",
    "                    ii = ii.strip(\"\\'\").strip(',').strip('[')\n",
    "                    parter.append(ii.strip(\"'\"))\n",
    "                    if ii not in tonghang:\n",
    "                        tonghang.append(ii.strip(',').strip(\"'\").strip('['))\n",
    "        mmm_list = []\n",
    "        for i in range(len(keywords_list)):\n",
    "            if keywords[0] in keywords_list[i]:\n",
    "                mmm_list.append(i)\n",
    "        for j in mmm_list:\n",
    "            ii = author_list[j]\n",
    "            if ii != '' and ii != name:\n",
    "                tonghang.append(ii.strip(',').strip(\"'\").strip('['))\n",
    "        if len(tonghang) >= 6:\n",
    "            tonghang = tonghang[-6:-1]\n",
    "        else:\n",
    "            tonghang = tonghang\n",
    "        if \"['']\" in tonghang:\n",
    "            tonghang.remove(\"['']\")\n",
    "        if ']' in tonghang:\n",
    "            tonghang.remove(']')\n",
    "        # 关键词\n",
    "        keyword = self.all_list(keywords)\n",
    "        keyword_s = []\n",
    "        for j in keyword.keys():\n",
    "            keyword_s.append(j)\n",
    "        if len(keyword_s) > 10:\n",
    "            keyword_s = keyword_s[0:10]\n",
    "        else:\n",
    "            keyword_s = keyword_s\n",
    "        # 期刊\n",
    "        jounals = self.all_list(jounal)\n",
    "        a_list = []\n",
    "        mm = sorted(jounals.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i in range(len(jounals)):\n",
    "            a_list.append(mm[i][0])\n",
    "        if len(a_list) > 3:\n",
    "            a_list = a_list[0:3]\n",
    "        else:\n",
    "            a_list = a_list\n",
    "        parters_index = self.all_list(parter)\n",
    "        if ']' in parters_index:\n",
    "            del parters_index[']']\n",
    "        if \"['']\" in parters_index:\n",
    "            del parters_index[\"['']\"]\n",
    "        mmmm = sorted(parters_index.items(), key=lambda x: x[1], reverse=True)\n",
    "        result = {'Author': self.name, 'Organ': organ, 'Partner': mmmm,\n",
    "                  'Journal': a_list, 'Papers': paper, 'Keywords': keyword_s, 'Peer': tonghang}\n",
    "        return result\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "class pre_word_count():\n",
    "    def __init__(self, data_path, model_path, hotword_num, hotyear_num):\n",
    "        self.data = pd.read_json(data_path)\n",
    "        self.data['authors_seg'] = self.data.apply(lambda r: r['P_Red1'] + r['P_Red3'], axis=1)\n",
    "        self.data['year'] = self.data['P_year'].apply(lambda x: x[:4])\n",
    "        self.model_path = model_path\n",
    "        self.hotword_num = hotword_num\n",
    "        self.hotyear_num = hotyear_num\n",
    "\n",
    "    # 统计出现在句子中的词频\n",
    "    def tf(self, word, sentence):\n",
    "        c = 0\n",
    "        s = sentence.split()\n",
    "        for i in s:\n",
    "            if word == i:\n",
    "                c += 1\n",
    "        return c\n",
    "\n",
    "    # 统计非重复的所有作者\n",
    "    def tongjiau(self, li):\n",
    "        authors = []\n",
    "        for i in li:\n",
    "            try:\n",
    "                authors += i.split()\n",
    "            except:\n",
    "                continue\n",
    "        return list(set(authors))\n",
    "\n",
    "    # 判断新作者是否在论文中提及热点词\n",
    "    def mention(self, authors, word, sentences, senten_auth):\n",
    "        i = 0\n",
    "        for a in authors:\n",
    "            for j in range(len(sentences)):\n",
    "                try:\n",
    "                    if len(list(set(a).intersection(set(senten_auth[j])))) > 0 and word in sentences[j]:\n",
    "                        i += 1\n",
    "                except:\n",
    "                    continue\n",
    "        return i\n",
    "\n",
    "    # 统计某个热点词在某年的不同期刊的提及数\n",
    "    def multi_journal(self, word, sentences, journals):\n",
    "        men_journal = []\n",
    "        for j in range(len(sentences)):\n",
    "            try:\n",
    "                if word in sentences[j]:\n",
    "                    men_journal.append(journals[j])\n",
    "            except:\n",
    "                continue\n",
    "        s = set(men_journal)\n",
    "        if None in s:\n",
    "            s.remove(None)\n",
    "        return len(s)\n",
    "\n",
    "    # 抽取词频特征\n",
    "    def feature(self, word, year_df):\n",
    "        abstract_tf = []\n",
    "        keyword_tf = []\n",
    "        for y in list(year_df['seg_abstract']):\n",
    "            abstract_tf.append(self.tf(word, y))\n",
    "        for i in list(year_df['seg_keywords']):\n",
    "            keyword_tf.append(self.tf(word, i))\n",
    "        return abstract_tf, keyword_tf\n",
    "\n",
    "    # 计算list的平均数\n",
    "\n",
    "    def pingjun(self, L):\n",
    "        if len(L) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            s = 0\n",
    "            for i in L:\n",
    "                s += i\n",
    "\n",
    "            return int(s / len(L))\n",
    "\n",
    "    # 获取热点词\n",
    "    def get_hotwords(self, hotword_num):\n",
    "        guanjian = ' '.join(list(self.data['P_Keyword_seg'])).split()\n",
    "        # textrank方法获取关键词\n",
    "\n",
    "        # merge=' '.join(guanjian)\n",
    "        # tr4w = TextRank4Keyword()\n",
    "        # tr4w.analyze(text=merge, window=4)\n",
    "        # pred_words=[]\n",
    "        # for item in tr4w.get_keywords(hotword_num, word_min_len=2):\n",
    "        # pred_words.append(item.word)\n",
    "        # 根据词频排序获取关键词\n",
    "        result = Counter(guanjian)\n",
    "        d = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "        pred_words = list(map(lambda x: x[0], d))[:hotword_num]\n",
    "        return pred_words\n",
    "\n",
    "    # 统计每年发表的论文数\n",
    "    def year_paper_num(self, ):\n",
    "        year_num = []\n",
    "        years = list(set(self.data['year']))\n",
    "        group_df = self.data.groupby(by=['year'])\n",
    "        # years.remove(None)\n",
    "        years = sorted([int(i) for i in years])\n",
    "        for y in years:\n",
    "            try:\n",
    "                year_num.append(len(group_df.get_group(str(y))))\n",
    "            except:\n",
    "                continue\n",
    "        return group_df, years, year_num\n",
    "\n",
    "    # 按年份将摘要整合\n",
    "    def groupby_year(self, group_df, years, year_num):\n",
    "        years_df = pd.DataFrame()\n",
    "        years_df['year'] = years\n",
    "        seg, key, tit, jou, authors = [], [], [], [], []\n",
    "\n",
    "        for i in years:\n",
    "            seg.append(' '.join(group_df.get_group(str(i))['P_Summary_seg']))\n",
    "            k = group_df.get_group(str(i))['P_Keyword_seg']\n",
    "            key.append(' '.join(k))\n",
    "            tit.append(' '.join(group_df.get_group(str(i))['P_Title_seg']))\n",
    "            jou.append(' '.join(group_df.get_group(str(i))['P_Publication']))\n",
    "            authors.append(self.tongjiau(group_df.get_group(str(i))['authors_seg']))\n",
    "        years_df['seg_abstract'] = seg\n",
    "        years_df['seg_keywords'] = key\n",
    "        years_df['seg_title'] = tit\n",
    "        years_df['seg_authors'] = authors\n",
    "        years_df['Jounal'] = jou\n",
    "        years_df['paper_num'] = year_num\n",
    "        return years_df\n",
    "\n",
    "    # 统计每年的新作者\n",
    "    def year_new_author(self, years_df):\n",
    "        authors = list(years_df['seg_authors'])\n",
    "        new_author = [authors[0]]\n",
    "        for i in range(1, len(years_df)):\n",
    "            new = []\n",
    "            for j in authors[i]:\n",
    "                if j not in authors[i - 1]:\n",
    "                    new.append(j)\n",
    "            new_author.append(new)\n",
    "        return new_author\n",
    "\n",
    "    # 统计每年新作者提及数\n",
    "    def year_new_author_mention(self, pred_words, new_author, years):\n",
    "        newauthor_mens = []\n",
    "        group_df = self.data.groupby(by=['year'])\n",
    "        for word in pred_words:\n",
    "            for i in range(len(new_author)):\n",
    "                newauthor_men = self.mention(new_author, word, list(group_df.get_group(str(years[i]))['P_Summary']), \\\n",
    "                                             list(map(lambda x: x.split(),\n",
    "                                                      list(group_df.get_group(str(years[i]))['authors_seg']))))\n",
    "                newauthor_mens.append(newauthor_men)\n",
    "        return newauthor_mens\n",
    "\n",
    "    # 统计跨学科提及数\n",
    "    def multi_sub(self, pred_words, years):\n",
    "        multi_mens = []\n",
    "        group_df = self.data.groupby(by=['year'])\n",
    "        for word in pred_words:\n",
    "            for i in range(len(years)):\n",
    "                journals = self.multi_journal(word, list(group_df.get_group(str(years[i]))['P_Summary']), \\\n",
    "                                              list(group_df.get_group(str(years[i]))['P_Publication']))\n",
    "                multi_mens.append(journals)\n",
    "        return multi_mens\n",
    "\n",
    "    # 统计每个词初次在关键词中出现的年份\n",
    "    def first_year(self, pred_words, years_df):\n",
    "        f_year = []\n",
    "        pp = []\n",
    "        for w in range(len(pred_words)):\n",
    "            for y, i in zip(list(years_df['year']), list(years_df['seg_keywords'])):\n",
    "                if pred_words[w] in i:\n",
    "                    f_year.append(y)\n",
    "                    pp.append(w)\n",
    "                    break\n",
    "        return f_year\n",
    "\n",
    "    def sub_df(self, pp, years_df, hot_year_num, hot_word_num, out_year, n_year):\n",
    "        year_num = len(years_df)\n",
    "        # 判断year组合是否有多余年份\n",
    "        pp_first = pp['word']\n",
    "        pp_first_line = []\n",
    "        for hy in range(hot_word_num):\n",
    "            for hyy in range(n_year):\n",
    "                pp_first_line.append(pp_first[hy*year_num])\n",
    "        pp_first_line = pd.DataFrame(pp_first_line)\n",
    "        pp_first_line.columns = ['word']\n",
    "        ppp = pp.drop(columns=['word'])\n",
    "        sub_dfq = {}\n",
    "        hot_nyear = []\n",
    "        for i in range(hot_word_num):\n",
    "            sub_dfq[i] = ppp.iloc[year_num * i+out_year: year_num * (i+1)]\n",
    "            sub_df_of_one = {}\n",
    "            for j in range(n_year):\n",
    "                sub_df_of_one[j] = sub_dfq[i].iloc[hot_year_num * j: hot_year_num * (j+1)]\n",
    "                sub_df_of_one[j].loc['Row'] = sub_df_of_one[j].apply(lambda x: x.sum())\n",
    "                hot_nyear.append(sub_df_of_one[j].loc['Row'])\n",
    "        df_hot_nyear = pd.DataFrame(hot_nyear)\n",
    "        df_hot_nyear.reset_index(inplace=True)\n",
    "        df_hot_nyear[\"word\"]=pp_first_line\n",
    "        df_hot_nyear=df_hot_nyear[[\"word\",\"f1\",\"f2\",\"f3\",\"f4\",\"f6\"]]\n",
    "        return df_hot_nyear\n",
    "\n",
    "    # 获取每个词18年的特征\n",
    "    def get_feature(self, years_df, pred_words, f_year, newauthor_mens, multi_mens):\n",
    "        year_num = len(years_df)\n",
    "        pp = pd.DataFrame()\n",
    "        for i in range(len(pred_words)):\n",
    "            word = pred_words[i]\n",
    "            fyear = f_year[i]\n",
    "            exist_year = []\n",
    "            for n, f in zip(list(years_df['year']), [fyear] * year_num):\n",
    "                e = n - f\n",
    "                if e < 0:\n",
    "                    exist_year.append(0)\n",
    "                else:\n",
    "                    exist_year.append(e)\n",
    "            dd = pd.DataFrame()\n",
    "            dd['word'] = [word] * year_num\n",
    "            dd['f1'] = self.feature(word, years_df)[0]\n",
    "            dd['f2'] = self.feature(word, years_df)[1]\n",
    "            dd['f3'] = exist_year\n",
    "            dd['f4'] = newauthor_mens[i * year_num:i * year_num + (year_num - 1)] + \\\n",
    "                       [self.pingjun(newauthor_mens[i * year_num + (year_num - 1) - 3:i * year_num + (year_num - 1)])]\n",
    "\n",
    "            dd['f6'] = multi_mens[i * year_num:i * year_num + (year_num - 1)] + \\\n",
    "                       [self.pingjun(multi_mens[i * year_num + (year_num - 1) - 3:i * year_num + (year_num - 1)])]\n",
    "\n",
    "            if i == 0:\n",
    "                pp = dd\n",
    "            else:\n",
    "                pp = pp.append(dd)\n",
    "        pp = pp.reset_index(drop=True)\n",
    "        return pp\n",
    "\n",
    "    # 预测某个词的词频\n",
    "    def zhidingword(self, search_word, test_data, model):\n",
    "        pre_data = test_data.loc[test_data.word == search_word].iloc[:, 1:]\n",
    "        result = model.predict(pre_data)\n",
    "        return int(result)\n",
    "\n",
    "    # 返回结果\n",
    "    def pre_result(self):\n",
    "        model_GBR = joblib.load(self.model_path)\n",
    "        pred_words = self.get_hotwords(self.hotword_num)\n",
    "        years = self.year_paper_num()[1]\n",
    "        group_df = self.year_paper_num()[0]\n",
    "        year_num = self.year_paper_num()[2]\n",
    "        years_df = self.groupby_year(group_df, years, year_num)\n",
    "        f_year = self.first_year(pred_words, years_df)\n",
    "        new_author = self.year_new_author(years_df)\n",
    "        newauthor_mens = self.year_new_author_mention(pred_words, new_author, years)\n",
    "        multi_mens = self.multi_sub(pred_words, years)\n",
    "        pp = self.get_feature(years_df, pred_words, f_year, newauthor_mens, multi_mens)\n",
    "\n",
    "        # out_year = len(years_df) % self.hotyear_num  ## 年余数\n",
    "        n_year = len(years_df) // self.hotyear_num  ## 多少组年\n",
    "        if n_year == 0:\n",
    "            out_year = len(years_df) % 1  ## 年余数\n",
    "            n_year = len(years_df) // 1  ## 多少组年\n",
    "            hotyear_numqq = 1\n",
    "        else:\n",
    "            out_year = len(years_df) % self.hotyear_num  ## 年余数\n",
    "            n_year = len(years_df) // self.hotyear_num  ## 多少组年\n",
    "            hotyear_numqq = self.hotyear_num\n",
    "\n",
    "        hot_nyear = self.sub_df(pp, years_df, hotyear_numqq, self.hotword_num, out_year, n_year)\n",
    "\n",
    "        num = list(range(n_year - 1, n_year * self.hotword_num, n_year))\n",
    "        test_data = hot_nyear.iloc[num]\n",
    "        all_wordtf = []\n",
    "        for word in pred_words:\n",
    "            result = self.zhidingword(word, test_data, model_GBR)\n",
    "            search_ke = list(hot_nyear.loc[hot_nyear.word == word]['f1']) + [result]\n",
    "            all_wordtf.extend(search_ke)\n",
    "        result = pd.DataFrame()\n",
    "        next_year = years[-1] + hotyear_numqq\n",
    "        years_nhotyear = []\n",
    "        for i in range(n_year):\n",
    "            years_nhotyear.append(years[i*hotyear_numqq+out_year+hotyear_numqq-1])\n",
    "        years_nhotyear.append(next_year)\n",
    "        wword_pred=[]\n",
    "        for w in pred_words:\n",
    "            for ww in years_nhotyear:\n",
    "                wword_pred.append(w)\n",
    "        result['word'] = wword_pred\n",
    "        result['year'] = years_nhotyear * len(pred_words)\n",
    "        result['count'] = all_wordtf\n",
    "        word_next_year = result.loc[result['year'] == next_year]\n",
    "        word_next_year_new = word_next_year.sort_values(by=\"count\", ascending=False)\n",
    "        Top_hot_word = list(word_next_year_new.loc[result['year'] == next_year]['word'])\n",
    "\n",
    "        hot_year = list(years_nhotyear)\n",
    "        freq_word = []\n",
    "        freq = []\n",
    "        freqs = []\n",
    "        for www in Top_hot_word:\n",
    "            freq_word.append(www)\n",
    "            freq = list(result.loc[result['word'] == www].sort_values(by=\"year\", ascending=True)['count'])\n",
    "            freqs.append(freq)\n",
    "        draw = pd.DataFrame()\n",
    "        draw[\"draw_word\"] = freq_word\n",
    "        draw[\"draw_freqs\"] = freqs\n",
    "        draw_dict = [{'draw_word': i[1], 'draw_freqs': i[2]} for i in draw.itertuples()]\n",
    "        #         draw_json=draw.to_json(orient='index')\n",
    "        altm = {'topnn': Top_hot_word, 'year': hot_year, 'draw': draw_dict}\n",
    "        return altm\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "def tongjiau(li):\n",
    "    authors = []\n",
    "    for i in li:\n",
    "        try:\n",
    "            authors += i\n",
    "        except:\n",
    "            continue\n",
    "    return list(set(authors))\n",
    "\n",
    "\n",
    "def savepickle(name, data):\n",
    "    output = open('%s.pkl' % (name), 'wb')\n",
    "    pkl.dump(data, output)\n",
    "    output.close()\n",
    "\n",
    "\n",
    "def loadpickle(name):\n",
    "    pkl_file = open('%s.pkl' % (name), 'rb')\n",
    "    data1 = pkl.load(pkl_file)\n",
    "    return data1\n",
    "\n",
    "\n",
    "def get_data(key, from_date, to_date, volume):\n",
    "    es4Paper = esPaperRetrieval(host='10.8.128.205', port=49200)\n",
    "    data = es4Paper.Retrieval(titleQuery=key, kwordQuery=key, summaryQuery=key, pubQuery='',\n",
    "                              fromDate=from_date, toDate=to_date, volume=volume)\n",
    "    data = pd.DataFrame(data)[[\"P_Author\", \"P_Publication\", \"P_Summary_seg\", \"P_Keyword\", \"P_year\"]]\n",
    "    data.columns = [\"authors\", \"Jounal\", \"seg_abstract\", \"keywords\", \"year\"]\n",
    "    data.year = [i[0:4] for i in data.year]\n",
    "\n",
    "    group_df = data.groupby(by=['year'])\n",
    "    # 统计每年的论文数量\n",
    "    years = list(set(data['year']))\n",
    "    # years.remove(None)\n",
    "    years = sorted([int(i) for i in years])\n",
    "    year_num = []\n",
    "    for y in years:\n",
    "        try:\n",
    "            year_num.append(len(group_df.get_group(str(y))))\n",
    "        except:\n",
    "            year_num.append(0)\n",
    "\n",
    "    # 根据年份将文献信息整合并保存\n",
    "\n",
    "    years_df = pd.DataFrame()\n",
    "    years_df['year'] = years\n",
    "    seg = []\n",
    "    key = []\n",
    "    jou = []\n",
    "    authors = []\n",
    "    for i in years:\n",
    "        seg.append(' '.join(list(group_df.get_group(str(i))['seg_abstract'])))\n",
    "        k = list(group_df.get_group(str(i))['keywords'])\n",
    "        if None in k:\n",
    "            while None in k:\n",
    "                k.remove(None)\n",
    "            key.append(' '.join(k))\n",
    "        else:\n",
    "            key.append(' '.join(k))\n",
    "        jou.append(' '.join(list(group_df.get_group(str(i))['Jounal'])))\n",
    "        authors.append(' '.join(tongjiau(list(group_df.get_group(str(i))['authors']))).split())\n",
    "    years_df['seg_abstract'] = seg\n",
    "    years_df['keywords'] = key\n",
    "    years_df['authors'] = authors\n",
    "    years_df['Jounal'] = jou\n",
    "    years_df['paper_num'] = year_num\n",
    "    savepickle('year_df', years_df)\n",
    "    return years_df\n",
    "\n",
    "\n",
    "def print_top_words_list(model, feature_names, n_top_words):\n",
    "    result = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        result += [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]]\n",
    "    return result\n",
    "\n",
    "\n",
    "def tf_lda_list(n_topic, n_top_words, max_feature, data):\n",
    "    vectorizer = CountVectorizer(max_features=max_feature)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    X = X.toarray()\n",
    "    # 主题词个数\n",
    "    n_top_words = n_top_words\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (n_samples, n_features))\n",
    "    lda = LatentDirichletAllocation(n_components=n_topic, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "\n",
    "    # print(\"\\nTopics in LDA model:\")\n",
    "    lda.fit(X)\n",
    "    tf_feature_names = vectorizer.get_feature_names()\n",
    "    return print_top_words_list(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "def zhutifenxi(key, n_topic=3, n_top_words=30, year_from=2010, year_to=2018, volume=500):\n",
    "    years_df = get_data(key, str(year_from - 1) + \"-12-31\", str(year_to) + \"-12-31\", volume)\n",
    "    max_feature = 2000\n",
    "    result = []\n",
    "    result += tf_lda_list(n_topic, n_top_words, max_feature,\n",
    "                          years_df.loc[years_df.year.isin(range(year_from, year_to + 1))]['seg_abstract'])\n",
    "    value = [[100] * n_top_words for i in range(n_topic)]\n",
    "    key = [[(\"name\", \"value\")] * n_top_words for i in range(n_topic)]\n",
    "\n",
    "    return [[dict(zip(t[0], t[1])) for t in k] for k in\n",
    "            [list(zip(j[0], j[1])) for j in list(zip(key, [list(zip(i[0], i[1])) for i in list(zip(result, value))]))]]\n",
    "\n",
    "\n",
    "def build_tfidf(max_feature, data):\n",
    "    #     vectorizer=CountVectorizer()\n",
    "    tfidf = TfidfVectorizer(max_features=max_feature)\n",
    "    f_tfidf = tfidf.fit_transform(data)\n",
    "    word = tfidf.get_feature_names()\n",
    "    f_tfidf = f_tfidf.toarray()\n",
    "    return f_tfidf, word\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    result_str = ''\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        result_str += \"Topic #%d:\\n\" % topic_idx\n",
    "        result_str += \" \".join([feature_names[i]\n",
    "                                for i in topic.argsort()[:-n_top_words - 1:-1]]) + '\\n'\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def tfidf_lda(n_topic, n_top_words, max_feature, data):\n",
    "    tfidf = TfidfVectorizer(max_features=max_feature)\n",
    "    f_tfidf = tfidf.fit_transform(data)\n",
    "    f_tfidf = f_tfidf.toarray()\n",
    "    # 主题词个数\n",
    "    n_top_words = n_top_words\n",
    "    n_samples = f_tfidf.shape[0]\n",
    "    n_features = f_tfidf.shape[1]\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_topic, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "\n",
    "    #     print(\"\\nTopics in LDA model:\")\n",
    "    lda.fit(f_tfidf)\n",
    "    tf_feature_names = tfidf.get_feature_names()\n",
    "    return print_top_words_list(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "def youxujulei(key, n_class=4, n_topic=1, n_top_topic=30):\n",
    "    years_df = get_data(key, \"2010-12-31\", \"2019-12-31\", 1000)\n",
    "    # 设置特征维度\n",
    "    max_feature = 2000\n",
    "    # 根据年份建立tfidf表\n",
    "    year_tfidf = build_tfidf(max_feature, years_df['seg_abstract'])\n",
    "    # 根据年份建立tf表\n",
    "    # year_tf=build_tf(max_feature,years_df['seg_abstract'])\n",
    "\n",
    "    # 有序聚类\n",
    "\n",
    "    # 聚类数量\n",
    "    lst = get_one_essay_trace(year_tfidf[0], n_class)\n",
    "    split_result = observe_divided(lst[0], years_df['year'])\n",
    "\n",
    "    split_df = pd.DataFrame()\n",
    "    abstract_agg = []\n",
    "    abstracts = []\n",
    "    for i in split_result:\n",
    "        tmp = []\n",
    "        for y in i:\n",
    "            tmp.append(list(years_df.loc[years_df.year == y]['seg_abstract'])[0])\n",
    "        abstract_agg.append(' '.join(tmp))\n",
    "        abstracts.append(tmp)\n",
    "    split_df['class'] = split_result\n",
    "    split_df['merge_abstract'] = abstract_agg\n",
    "    split_df['abstracts'] = abstracts\n",
    "\n",
    "    # 根据tf计算LDA主题词\n",
    "    # 算法 类名/第i类\n",
    "    result_str = []\n",
    "    keys = [\"pub_id\", \"title\", \"publicationDate\", \"firstApplicant\"]\n",
    "    for i in range(n_class):\n",
    "        t = 1\n",
    "        for j in tf_lda_list(n_topic, n_top_topic, max_feature, split_df.loc[i, 'abstracts']):\n",
    "            result_str += [dict(zip(keys, [\"tf-lda\", str(split_df['class'][i]) + \"年\", \"主题\" + str(t), \" \".join(j)]))]\n",
    "            t += 1\n",
    "\n",
    "    # 根据tfidf计算LDA主题词\n",
    "    for i in range(n_class):\n",
    "        t = 1\n",
    "        for j in tfidf_lda(n_topic, n_top_topic, max_feature, split_df.loc[i, 'abstracts']):\n",
    "            result_str += [dict(zip(keys, [\"tfidf-lda\", \"第\" + str(i + 1) + \"类\", \"主题\" + str(t), \" \".join(j)]))]\n",
    "            t += 1\n",
    "\n",
    "    return result_str\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "def distance(a, b):\n",
    "    #  计算两个vector的距离，逐项差的平方和，不开根号\n",
    "    if len(a) != len(b):\n",
    "        print('wrong')\n",
    "        return 0\n",
    "    else:\n",
    "        temp = a - b\n",
    "        temp = temp ** 2\n",
    "        res = sum(temp)\n",
    "        return res\n",
    "def get_dist_mat(feat):\n",
    "    #  输入为特征矩阵，返回一个相似度矩阵（行数乘行数）\n",
    "    print(type(feat))\n",
    "    length=feat.shape[0]#样本数量\n",
    "    wide = feat.shape[1]#特征向量长度\n",
    "    res = np.zeros(shape=(length, length))\n",
    "    ave = np.zeros(shape=(length, length, wide))\n",
    "\n",
    "    for i in range(0, length):\n",
    "        for j in range(i+1, length):\n",
    "            ave[i, j] = np.mean(feat[i: j+1], axis=0)\n",
    "            # print('i = %d, j = %d' % (i, j))\n",
    "\n",
    "    for i in range(0, length):\n",
    "        for j in range(i+1, length):\n",
    "            ave_one = ave[i, j]\n",
    "            dist_lst = [distance(ave_one, one) for one in feat[i: j+1]]\n",
    "            res[i, j] = sum(dist_lst)\n",
    "            # print('i = %d, j = %d' % (i, j))\n",
    "    return res\n",
    "\n",
    "def get_class_devide(dist_mat, class_num):\n",
    "    #  利用递推公式计算分成class_num类别的分割方法\n",
    "    length = len(dist_mat)\n",
    "    divided_point = np.zeros(shape=(length, class_num+1))  # 存切割点\n",
    "    diveded_dist = np.zeros(shape=(length, class_num+1))  # 存切割最小距离\n",
    "\n",
    "    for i in range(1, length):  # 分成两类\n",
    "        dist_lst = [dist_mat[0, k] + dist_mat[k+1, i] for k in range(0, i)]\n",
    "        divided_point[i, 2] = np.argmin(dist_lst)\n",
    "        diveded_dist[i, 2] = np.min(dist_lst)\n",
    "\n",
    "    for classes in range(3, class_num+1):   # 分成多类\n",
    "        for i in range(classes-1, length):\n",
    "            dist_lst = [diveded_dist[k, classes-1] + dist_mat[k+1, i] for k in range(classes-2, i)]  # 从n-1类到n类\n",
    "            divided_point[i, classes] = np.argmin(dist_lst) + classes - 2\n",
    "            diveded_dist[i, classes] = np.min(dist_lst)\n",
    "    return diveded_dist, divided_point\n",
    "\n",
    "def get_trace(trace_mat, class_num):\n",
    "    #  获得切割方法list，\n",
    "    #  trace_mat就是get_class_devide函数返回的divided_point，分割点记录矩阵\n",
    "    lst = []\n",
    "    length = len(trace_mat)  #length=38\n",
    "    pre = length - 1   #pre=37\n",
    "    for i in range(class_num, 1, -1):\n",
    "        temp = int(trace_mat[pre, i])\n",
    "        pre = temp\n",
    "        lst.append(temp)\n",
    "    lst.reverse()\n",
    "    return lst\n",
    "def get_one_essay_trace(feature,phrase_num):\n",
    "    #  整合调用 有序聚类方法\n",
    "    #  调用就可以获得一个文件的分割结果并存入本地\n",
    "\n",
    "    time_start = time.time()\n",
    "    dist_mat = get_dist_mat(feature)\n",
    "    phrase_num = phrase_num\n",
    "    min_dist_mat, min_trace_mat = get_class_devide(dist_mat, phrase_num)\n",
    "    # print(min_trace_mat)\n",
    "\n",
    "    lst = get_trace(min_trace_mat, phrase_num)\n",
    "    lst.append(len(feature))\n",
    "    print('划分结果为：',end='')\n",
    "    print(lst)\n",
    "    time_end = time.time()\n",
    "    print('cost time', time_end - time_start)\n",
    "    return lst,min_dist_mat,min_trace_mat\n",
    "def observe_divided(lst, sents):\n",
    "    # 展示分割结果\n",
    "    split_result=[]\n",
    "    start = 0\n",
    "    for end in lst:\n",
    "        cla_lst=[]\n",
    "        if end==len(sents):\n",
    "            end=end-1\n",
    "        for i in range(start, end+1):\n",
    "            cla_lst.append(sents[i])\n",
    "        start = end + 1\n",
    "        split_result.append(cla_lst)\n",
    "    return split_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T02:09:55.534049Z",
     "start_time": "2019-07-25T02:09:53.050103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawbda/env/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:445: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topnn': ['中医药', '中医医院', '中医', '医院', '医疗设备', '影响因素', '公立医院', '医患关系', '医联体', '管理', '满意度', '中医药法', '分级诊疗', '对策', '实践'], 'year': [2018, 2020], 'draw': [{'draw_word': '中医药', 'draw_freqs': [213, 365]}, {'draw_word': '中医医院', 'draw_freqs': [236, 303]}, {'draw_word': '中医', 'draw_freqs': [472, 274]}, {'draw_word': '医院', 'draw_freqs': [747, 240]}, {'draw_word': '医疗设备', 'draw_freqs': [70, 129]}, {'draw_word': '影响因素', 'draw_freqs': [74, 129]}, {'draw_word': '公立医院', 'draw_freqs': [95, 110]}, {'draw_word': '医患关系', 'draw_freqs': [183, 96]}, {'draw_word': '医联体', 'draw_freqs': [122, 89]}, {'draw_word': '管理', 'draw_freqs': [97, 61]}, {'draw_word': '满意度', 'draw_freqs': [60, 35]}, {'draw_word': '中医药法', 'draw_freqs': [48, 21]}, {'draw_word': '分级诊疗', 'draw_freqs': [30, 21]}, {'draw_word': '对策', 'draw_freqs': [38, 20]}, {'draw_word': '实践', 'draw_freqs': [45, 19]}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "esPaperObj = esPaperRetrieval(host='10.8.128.205',port=49200)\n",
    "docs = esPaperObj.Retrieval(titleQuery='',kwordQuery='',summaryQuery='医',pubQuery='', fromDate='2016-01-01',toDate='2019-01-01',volume=1000)\n",
    "\n",
    "hotPointObj = pre_word_count(json.dumps(docs), '../data/paper_data/paper_hotpoint.model',15, 2)\n",
    "h=hotPointObj.pre_result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T02:10:03.391026Z",
     "start_time": "2019-07-25T02:10:03.377064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topnn': ['中医药',\n",
       "  '中医医院',\n",
       "  '中医',\n",
       "  '医院',\n",
       "  '医疗设备',\n",
       "  '影响因素',\n",
       "  '公立医院',\n",
       "  '医患关系',\n",
       "  '医联体',\n",
       "  '管理',\n",
       "  '满意度',\n",
       "  '中医药法',\n",
       "  '分级诊疗',\n",
       "  '对策',\n",
       "  '实践'],\n",
       " 'year': [2018, 2020],\n",
       " 'draw': [{'draw_word': '中医药', 'draw_freqs': [213, 365]},\n",
       "  {'draw_word': '中医医院', 'draw_freqs': [236, 303]},\n",
       "  {'draw_word': '中医', 'draw_freqs': [472, 274]},\n",
       "  {'draw_word': '医院', 'draw_freqs': [747, 240]},\n",
       "  {'draw_word': '医疗设备', 'draw_freqs': [70, 129]},\n",
       "  {'draw_word': '影响因素', 'draw_freqs': [74, 129]},\n",
       "  {'draw_word': '公立医院', 'draw_freqs': [95, 110]},\n",
       "  {'draw_word': '医患关系', 'draw_freqs': [183, 96]},\n",
       "  {'draw_word': '医联体', 'draw_freqs': [122, 89]},\n",
       "  {'draw_word': '管理', 'draw_freqs': [97, 61]},\n",
       "  {'draw_word': '满意度', 'draw_freqs': [60, 35]},\n",
       "  {'draw_word': '中医药法', 'draw_freqs': [48, 21]},\n",
       "  {'draw_word': '分级诊疗', 'draw_freqs': [30, 21]},\n",
       "  {'draw_word': '对策', 'draw_freqs': [38, 20]},\n",
       "  {'draw_word': '实践', 'draw_freqs': [45, 19]}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
