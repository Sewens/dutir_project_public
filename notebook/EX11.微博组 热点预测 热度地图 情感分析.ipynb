{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、热度地图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:16:19.470319Z",
     "start_time": "2019-07-24T11:16:19.026739Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pymysql\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:18:44.929285Z",
     "start_time": "2019-07-24T11:18:22.109706Z"
    }
   },
   "outputs": [],
   "source": [
    "table_name = 'weibo_table'\n",
    "create_time_name = 'W_created_at'\n",
    "host = '10.8.128.205'\n",
    "port = 23306\n",
    "user = 'weibo_user'\n",
    "password = 'ir@DUT#weibo'\n",
    "db_name = 'weiboDB'\n",
    "#load dicts\n",
    "jieba.load_userdict(\"../data/weibo_data/dicts/SogouLabDic.txt\")\n",
    "jieba.load_userdict(\"../data/weibo_data/dicts/dict_baidu_utf8.txt\")\n",
    "jieba.load_userdict(\"../data/weibo_data/dicts/dict_pangu.txt\")\n",
    "jieba.load_userdict(\"../data/weibo_data/dicts/dict_sougou_utf8.txt\")\n",
    "jieba.load_userdict(\"../data/weibo_data/dicts/dict_tencent_utf8.txt\")\n",
    "jieba.load_userdict(\"../data/weibo_data/dicts/my_dict.txt\")\n",
    "stopwords = {}.fromkeys([ line.rstrip() for line in open('../data/weibo_data/dicts/Stopword.txt','r',encoding='utf-8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:18:47.723524Z",
     "start_time": "2019-07-24T11:18:47.694720Z"
    }
   },
   "outputs": [],
   "source": [
    "# input：直接从数据库查询 tag =0时输出所有微博计算的热度地图，tag为其他值，则计算tag小时内微博计算的热度地图\n",
    "# output：输出标准化后的热度地图字典\n",
    "def get_map_number(tag):\n",
    "    dict_dir = ''\n",
    "    jieba.load_userdict(os.path.join(dict_dir,\"SogouLabDic.txt\"))\n",
    "    jieba.load_userdict(os.path.join(dict_dir,\"dict_baidu_utf8.txt\"))\n",
    "    jieba.load_userdict(os.path.join(dict_dir,\"dict_pangu.txt\"))\n",
    "    jieba.load_userdict(os.path.join(dict_dir,\"dict_sougou_utf8.txt\"))\n",
    "    jieba.load_userdict(os.path.join(dict_dir,\"dict_tencent_utf8.txt\"))\n",
    "    jieba.load_userdict(os.path.join(dict_dir,\"my_dict.txt\"))\n",
    "    table_name = 'weibo_table'\n",
    "    create_time_name = 'W_created_at'\n",
    "    host = '10.8.128.205'\n",
    "    port = 23306\n",
    "    user = 'weibo_user'\n",
    "    password = 'ir@DUT#weibo'\n",
    "    db_name = 'weiboDB'\n",
    "    stopwords = {}.fromkeys([ line.rstrip() for line in open('../data/weibo_data/dicts/Stopword.txt','r',encoding='utf-8')])\n",
    "    db = pymysql.connect(host = host,port = port,user = user,password = password,db = db_name,charset='utf8',cursorclass = pymysql.cursors.DictCursor)\n",
    "    cursor = db.cursor()\n",
    "    if tag != 0:\n",
    "        # tag小时微博\n",
    "        sql_1 = \"select W_content from \" + str(table_name) + \" where \" + str(create_time_name) + \">=(now() - interval \" + str(tag) +\" hour)\"\n",
    "    else:\n",
    "        # 所有微博\n",
    "        sql_1 = \"select W_content from \" + str(table_name)\n",
    "    query = cursor.execute(sql_1)\n",
    "#     print(query)\n",
    "    result = []\n",
    "    result_middle_1 = []\n",
    "    result_middle = []\n",
    "    lag = 0\n",
    "    if query > 0:\n",
    "        result_middle_1 = cursor.fetchall()\n",
    "        for i in range(1,len(result_middle_1)):\n",
    "            result_middle = result_middle_1[i]['W_content']\n",
    "            seg = jieba.cut(result_middle)\n",
    "            for i in seg:\n",
    "                if i not in stopwords:  \n",
    "                    result.append(i)\n",
    "    else:\n",
    "        lag = 1\n",
    "    db.close()\n",
    "\n",
    "    # 对筛选时间段内的微博统计地域信息\n",
    "    if lag != 1: \n",
    "        provin = '北京市，天津市，上海市，重庆市，河北省，山西省，辽宁省，吉林省，黑龙江省，江苏省，浙江省，安徽省，福建省，江西省，山东省，河南省，湖北省，湖南省，广东省，海南省，四川省，贵州省，云南省，陕西省，甘肃省，青海省，台湾省，内蒙古自治区，广西壮族自治区，西藏自治区，宁夏回族自治区，新疆维吾尔自治区，香港特别行政区，澳门特别行政区，北京，天津，上海，重庆，河北，山西，辽宁，吉林，黑龙江，江苏，浙江，安徽，福建，江西，山东，河南，湖北，湖南，广东，海南，四川，贵州，云南，陕西，甘肃，青海，台湾，内蒙古，广西，西藏，宁夏，新疆，香港，澳门'\n",
    "        provin = provin.split('，')\n",
    "        provin1 = '北京，天津，上海，重庆，河北，山西，辽宁，吉林，黑龙江，江苏，浙江，安徽，福建，江西，山东，河南，湖北，湖南，广东，海南，四川，贵州，云南，陕西，甘肃，青海，台湾，内蒙古，广西，西藏，宁夏，新疆，香港，澳门'\n",
    "        provin2 = '北京市，天津市，上海市，重庆市，河北省，山西省，辽宁省，吉林省，黑龙江省，江苏省，浙江省，安徽省，福建省，江西省，山东省，河南省，湖北省，湖南省，广东省，海南省，四川省，贵州省，云南省，陕西省，甘肃省，青海省，台湾省，内蒙古自治区，广西壮族自治区，西藏自治区，宁夏回族自治区，新疆维吾尔自治区，香港特别行政区，澳门特别行政区'\n",
    "        provin1 = provin1.split('，')\n",
    "        provin2 = provin2.split('，')\n",
    "        count = {}\n",
    "        count2 = {}\n",
    "        # get the weibo numbers of all the cities\n",
    "        for word in result:\n",
    "            if word in provin:\n",
    "                count[word] = count.get(word, 0) + 1\n",
    "        for i in zip(provin1, provin2):\n",
    "            count2[i[0]] = count.get(i[0], 0) + count.get(i[1], 0)\n",
    "\n",
    "        # scale the numbers in 0-100\n",
    "        values = []\n",
    "        for i in count2.values():\n",
    "            values.append(i)\n",
    "        values = np.array(values)\n",
    "        try:\n",
    "            Max = values.max()\n",
    "            Min = values.min()\n",
    "            if Max != Min:\n",
    "                values = ((values - Min) / (Max - Min) * 100).astype(np.int)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "        values = values.tolist()\n",
    "\n",
    "        # get the final result as [['北京', 46], ['天津', 5],...]\n",
    "        provin_list = []\n",
    "        for key, value in zip(provin1, values):\n",
    "            list1 = [key, value]\n",
    "            provin_list.append(list1)\n",
    "        provin_dict = dict()        \n",
    "        for i in provin_list:        \n",
    "            provin_dict[i[0]]=i[1]\n",
    "    else:\n",
    "        return 'No weibo contains!'\n",
    "    return provin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:59:25.062605Z",
     "start_time": "2019-07-24T11:59:18.113127Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = get_map_number(0)\n",
    "\n",
    "result = [{'name':str(k),'value':float(result[k])} for k in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:59:25.164374Z",
     "start_time": "2019-07-24T11:59:25.071355Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': '北京', 'value': 70.0},\n",
       " {'name': '天津', 'value': 5.0},\n",
       " {'name': '上海', 'value': 100.0},\n",
       " {'name': '重庆', 'value': 4.0},\n",
       " {'name': '河北', 'value': 1.0},\n",
       " {'name': '山西', 'value': 2.0},\n",
       " {'name': '辽宁', 'value': 4.0},\n",
       " {'name': '吉林', 'value': 1.0},\n",
       " {'name': '黑龙江', 'value': 1.0},\n",
       " {'name': '江苏', 'value': 10.0},\n",
       " {'name': '浙江', 'value': 20.0},\n",
       " {'name': '安徽', 'value': 2.0},\n",
       " {'name': '福建', 'value': 1.0},\n",
       " {'name': '江西', 'value': 6.0},\n",
       " {'name': '山东', 'value': 9.0},\n",
       " {'name': '河南', 'value': 5.0},\n",
       " {'name': '湖北', 'value': 0.0},\n",
       " {'name': '湖南', 'value': 2.0},\n",
       " {'name': '广东', 'value': 4.0},\n",
       " {'name': '海南', 'value': 1.0},\n",
       " {'name': '四川', 'value': 2.0},\n",
       " {'name': '贵州', 'value': 9.0},\n",
       " {'name': '云南', 'value': 4.0},\n",
       " {'name': '陕西', 'value': 11.0},\n",
       " {'name': '甘肃', 'value': 2.0},\n",
       " {'name': '青海', 'value': 1.0},\n",
       " {'name': '台湾', 'value': 12.0},\n",
       " {'name': '内蒙古', 'value': 1.0},\n",
       " {'name': '广西', 'value': 0.0},\n",
       " {'name': '西藏', 'value': 3.0},\n",
       " {'name': '宁夏', 'value': 0.0},\n",
       " {'name': '新疆', 'value': 1.0},\n",
       " {'name': '香港', 'value': 88.0},\n",
       " {'name': '澳门', 'value': 1.0}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、评论热度分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:19:29.871637Z",
     "start_time": "2019-07-24T11:19:29.259010Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "body = {'wordQuery':'',\n",
    "        'textQuery':'垃圾分类',\n",
    "        'volume':1000}\n",
    "baseUrl = 'http://10.8.128.205:29280/Lawbda/dataWare/1.0.0/weibo/search'\n",
    "docs = requests.get(baseUrl,params=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:42:26.233692Z",
     "start_time": "2019-07-24T11:42:26.209623Z"
    }
   },
   "outputs": [],
   "source": [
    "class esWeiboCommentRetrieval():\n",
    "    '''\n",
    "    根据论文检索需求进行功能的微调\n",
    "    '''\n",
    "    _instance = None\n",
    "    _first_init = True\n",
    "\n",
    "    def __new__(cls, *args, **kw):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(esWeiboCommentRetrieval, cls).__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self, host, port):\n",
    "        '''\n",
    "        使用ES进行论文检索 指定host、port以及专利index之后进行检索\n",
    "        '''\n",
    "        super(esWeiboCommentRetrieval, self).__init__()\n",
    "        self.es = Elasticsearch(hosts=host, port=port, timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "        self.indexName = 'weibo-comment-index'\n",
    "\n",
    "    def do_search(self, wordQuery, textQuery, volume):\n",
    "        '''\n",
    "        do_search方法执行具体检索过程\n",
    "        wordQuery 本应为查询对应微博时所用的检索词 此处暂时不用 目前暂时只检索微博评论数据后续再修改对象为\n",
    "\n",
    "        volume为每次检索返回的数目\n",
    "        '''\n",
    "        queryBody = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"C_content\": textQuery\n",
    "                }\n",
    "            },\n",
    "            \"from\": 0,\n",
    "            \"size\": volume,\n",
    "            \"sort\": [],\n",
    "            \"aggs\": {}\n",
    "        }\n",
    "        result = self.es.search(index=self.indexName, body=queryBody)\n",
    "        return result\n",
    "\n",
    "    def format_search(self, result):\n",
    "        '''\n",
    "        format_search方法对检索结果进行格式化 构建符合要求的字段进行返回\n",
    "        输入result为检索结果 提取其中的检索结果进行后处理\n",
    "        使用ES检索后得到的结果中result['hits']['hits']为数组格式数据\n",
    "        其中每一个元素为一个dict 对应部分字段\n",
    "        '''\n",
    "        docs = result['hits']['hits']\n",
    "        docs = [i['_source'] for i in docs]\n",
    "        targetKeyList = 'C_ID,C_comment_id,C_comment_user_id,C_comment_user_nick_name,C_content,\\\n",
    "        C_weibo_url,C_like_num,C_created_at,C_crawl_time'\n",
    "        targetKeyList = [i.strip() for i in targetKeyList.split(',')]\n",
    "        dict_filter_by_keys = lambda d: {k: d[k] for k in targetKeyList}\n",
    "        dict_filter_text = lambda d: {k if not k == 'text' else 'claim_text': d[k] for k in d}\n",
    "        dict_filter_id = lambda d: {k if not k == '_id' else 'id': d[k] for k in d}\n",
    "        docs = (dict_filter_by_keys(doc) for doc in docs)\n",
    "        docs = [dict_filter_text(doc) for doc in docs]\n",
    "        return docs\n",
    "\n",
    "    def Retrieval(self, wordQuery, textQuery, volume):\n",
    "        result = self.do_search(wordQuery, textQuery, volume)\n",
    "        docs = self.format_search(result)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:44:38.366859Z",
     "start_time": "2019-07-24T11:44:38.354725Z"
    }
   },
   "outputs": [],
   "source": [
    "# 输入: 评论查询结果docs及参数time_period = ''\n",
    "# 输出时间段评论数量 period_result  例如  {'2015': 1, '2016': 0, '2017': 21, '2018': 109, '2019': 869}\n",
    "\n",
    "def get_comment_num(docs,time_period):\n",
    "    # 按年统计并显示  time_period = 'A'\n",
    "    # 按季度统计并显示 time_period = 'Q'\n",
    "    # 按月度统计并显示 time_period = 'M'\n",
    "    # 按周统计并显示 time_period = 'W'\n",
    "    # 按天统计并显示 time_period = 'D'\n",
    "    # 按小时统计并显示 time_period = '12h'\n",
    "    # 按分钟统计并显示 time_period = '60min'\n",
    "    data=docs\n",
    "    time_list =[i['C_created_at'].replace('T',' ') for i in docs]\n",
    "    time_result={'date':time_list,'num':1}\n",
    "    data=pd.DataFrame(time_result)\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    df = data.set_index('date')\n",
    "    result = df.resample(time_period).sum().to_period(time_period)\n",
    "    num_sum = [int(np.squeeze(i)) for i in result.values]\n",
    "    date_split = [str(i) for i in result.index]\n",
    "    period_result = dict(zip(date_split, num_sum))\n",
    "    return period_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:44:20.874646Z",
     "start_time": "2019-07-24T11:44:20.869731Z"
    }
   },
   "outputs": [],
   "source": [
    "esObj = esWeiboCommentRetrieval(host='10.8.128.205',port=49200)\n",
    "\n",
    "docs = esObj.Retrieval(wordQuery='',textQuery='垃圾分类',volume=100)\n",
    "\n",
    "result = get_comment_num(docs, time_period = '2m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:45:27.581233Z",
     "start_time": "2019-07-24T11:45:27.573861Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2017-12': 3,\n",
       " '2018-02': 0,\n",
       " '2018-04': 4,\n",
       " '2018-06': 2,\n",
       " '2018-08': 5,\n",
       " '2018-10': 0,\n",
       " '2018-12': 6,\n",
       " '2019-02': 1,\n",
       " '2019-04': 0,\n",
       " '2019-06': 1,\n",
       " '2019-08': 78}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:45:49.540942Z",
     "start_time": "2019-07-24T11:45:49.535316Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = [{'name':str(k),'value':float(result[k])} for k in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、返回查询词对应微博"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:19:31.794046Z",
     "start_time": "2019-07-24T11:19:31.765581Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:19:32.703808Z",
     "start_time": "2019-07-24T11:19:32.678566Z"
    }
   },
   "outputs": [],
   "source": [
    "class esWeiboTweetRetrieval():\n",
    "    '''\n",
    "    根据论文检索需求进行功能的微调\n",
    "    '''\n",
    "    _instance = None\n",
    "    _first_init = True\n",
    "    def __new__(cls, *args, **kw):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(esWeiboTweetRetrieval, cls).__new__(cls)  \n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, host, port):\n",
    "        '''\n",
    "        使用ES进行论文检索 指定host、port以及专利index之后进行检索\n",
    "        '''\n",
    "        super(esWeiboTweetRetrieval, self).__init__()\n",
    "        self.es = Elasticsearch(hosts=host, port=port, timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "        self.indexName = 'weibo-tweet-index'\n",
    "\n",
    "    def do_search(self, W_search_query, W_content, volume):\n",
    "        '''\n",
    "        do_search方法执行具体检索过程\n",
    "        wordQuery 本应为查询对应微博时所用的检索词 此处暂时不用 目前暂时只检索微博评论数据后续再修改对象为\n",
    "        \n",
    "        volume为每次检索返回的数目\n",
    "        '''\n",
    "        queryBody = {\n",
    "        \"query\": {\n",
    "        \"bool\": {\n",
    "        \"must\": [ ],\n",
    "        \"must_not\": [ ],\n",
    "        \"should\": [\n",
    "        {\n",
    "        \"query_string\": {\n",
    "        \"default_field\": \"W_content\",\n",
    "        \"query\": W_content\n",
    "        }\n",
    "        }\n",
    "        ,\n",
    "        {\n",
    "        \"query_string\": {\n",
    "        \"default_field\": \"W_search_query\",\n",
    "        \"query\": W_search_query\n",
    "        }\n",
    "        }\n",
    "        ]\n",
    "        }\n",
    "        },\n",
    "        \"from\": 0,\n",
    "        \"size\": volume,\n",
    "        \"sort\": [ ],\n",
    "        \"aggs\": { }\n",
    "        }\n",
    "        result = self.es.search(index=self.indexName, body=queryBody)\n",
    "        return result\n",
    "\n",
    "    def format_search(self, result):\n",
    "        '''\n",
    "        format_search方法对检索结果进行格式化 构建符合要求的字段进行返回\n",
    "        输入result为检索结果 提取其中的检索结果进行后处理\n",
    "        使用ES检索后得到的结果中result['hits']['hits']为数组格式数据\n",
    "        其中每一个元素为一个dict 对应部分字段\n",
    "        '''\n",
    "        docs = result['hits']['hits']\n",
    "        docs = [i['_source'] for i in docs]\n",
    "        targetKeyList = 'W_ID,W_weibo_id,W_search_query,W_nick_name,W_weibo_url,W_created_at,\\\n",
    "        W_like_num,W_repost_num,W_comment_num,W_content,W_user_id,W_image_url,W_video_url,W_tool,W_location,\\\n",
    "        W_location_map_info,W_origin_weibo_url,W_origin_weibo_content,W_crawl_time'\n",
    "        targetKeyList = [i.strip() for i in targetKeyList.split(',')]\n",
    "        dict_filter_by_keys = lambda d: {k: d[k] for k in targetKeyList}\n",
    "        dict_filter_id = lambda d: {k if not k == '_id' else 'id': d[k] for k in d}\n",
    "        docs = list(dict_filter_by_keys(doc) for doc in docs)\n",
    "        return docs\n",
    "    def Retrieval(self, W_search_query, W_content, volume):\n",
    "        result = self.do_search(W_search_query, W_content, volume)\n",
    "        docs = self.format_search(result)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:19:33.830186Z",
     "start_time": "2019-07-24T11:19:33.824583Z"
    }
   },
   "outputs": [],
   "source": [
    "eee = esWeiboTweetRetrieval(host='10.8.128.205',port=49200,)\n",
    "\n",
    "result= eee.Retrieval(W_search_query='#垃圾分类#', W_content='北京',volume=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:19:36.808764Z",
     "start_time": "2019-07-24T11:19:36.798297Z"
    }
   },
   "outputs": [],
   "source": [
    "# input 微博检索结果\n",
    "def get_query_weibo(result):\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_query_weibo(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from jieba import analyse\n",
    "import pickle\n",
    "import os\n",
    "import jieba\n",
    "from jieba import analyse\n",
    "from snownlp import SnowNLP\n",
    "from snownlp.sentiment import Sentiment\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:49:14.905432Z",
     "start_time": "2019-07-24T11:49:14.882627Z"
    }
   },
   "outputs": [],
   "source": [
    "class esWeiboTweetRetrieval():\n",
    "    '''\n",
    "    根据论文检索需求进行功能的微调\n",
    "    '''\n",
    "    _instance = None\n",
    "    _first_init = True\n",
    "    def __new__(cls, *args, **kw):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(esWeiboTweetRetrieval, cls).__new__(cls)  \n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, host, port):\n",
    "        '''\n",
    "        使用ES进行论文检索 指定host、port以及专利index之后进行检索\n",
    "        '''\n",
    "        super(esWeiboTweetRetrieval, self).__init__()\n",
    "        self.es = Elasticsearch(hosts=host, port=port, timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "        self.indexName = 'weibo-tweet-index'\n",
    "\n",
    "    def do_search(self, W_search_query, W_content, volume):\n",
    "        '''\n",
    "        do_search方法执行具体检索过程\n",
    "        wordQuery 本应为查询对应微博时所用的检索词 此处暂时不用 目前暂时只检索微博评论数据后续再修改对象为\n",
    "        \n",
    "        volume为每次检索返回的数目\n",
    "        '''\n",
    "        queryBody = {\n",
    "        \"query\": {\n",
    "        \"bool\": {\n",
    "        \"must\": [ ],\n",
    "        \"must_not\": [ ],\n",
    "        \"should\": [\n",
    "        {\n",
    "        \"query_string\": {\n",
    "        \"default_field\": \"W_content\",\n",
    "        \"query\": W_content\n",
    "        }\n",
    "        }\n",
    "        ,\n",
    "        {\n",
    "        \"query_string\": {\n",
    "        \"default_field\": \"W_search_query\",\n",
    "        \"query\": W_search_query\n",
    "        }\n",
    "        }\n",
    "        ]\n",
    "        }\n",
    "        },\n",
    "        \"from\": 0,\n",
    "        \"size\": volume,\n",
    "        \"sort\": [ ],\n",
    "        \"aggs\": { }\n",
    "        }\n",
    "        result = self.es.search(index=self.indexName, body=queryBody)\n",
    "        return result\n",
    "\n",
    "    def format_search(self, result):\n",
    "        '''\n",
    "        format_search方法对检索结果进行格式化 构建符合要求的字段进行返回\n",
    "        输入result为检索结果 提取其中的检索结果进行后处理\n",
    "        使用ES检索后得到的结果中result['hits']['hits']为数组格式数据\n",
    "        其中每一个元素为一个dict 对应部分字段\n",
    "        '''\n",
    "        docs = result['hits']['hits']\n",
    "        docs = [i['_source'] for i in docs]\n",
    "        targetKeyList = 'W_ID,W_weibo_id,W_search_query,W_nick_name,W_weibo_url,W_created_at,\\\n",
    "        W_like_num,W_repost_num,W_comment_num,W_content,W_user_id,W_image_url,W_video_url,W_tool,W_location,\\\n",
    "        W_location_map_info,W_origin_weibo_url,W_origin_weibo_content,W_crawl_time'\n",
    "        targetKeyList = [i.strip() for i in targetKeyList.split(',')]\n",
    "        dict_filter_by_keys = lambda d: {k: d[k] for k in targetKeyList}\n",
    "        dict_filter_id = lambda d: {k if not k == '_id' else 'id': d[k] for k in d}\n",
    "        docs = list(dict_filter_by_keys(doc) for doc in docs)\n",
    "        return docs\n",
    "    def Retrieval(self, W_search_query, W_content, volume):\n",
    "        result = self.do_search(W_search_query, W_content, volume)\n",
    "        docs = self.format_search(result)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:52:54.714359Z",
     "start_time": "2019-07-24T11:52:54.661323Z"
    }
   },
   "outputs": [],
   "source": [
    "# input： 微博检索结果\n",
    "# output： {\n",
    "#         dict_senti          微博情感得分\n",
    "#         pos_content[0:5]    正向微博前5条\n",
    "#         neg_content[0:5]    负向微博前5条\n",
    "# }\n",
    "\n",
    "def get_weibo_senti(result):\n",
    "    happy = [\"PA\",\"PE\"]#乐\n",
    "    like = [\"PD\",\"PH\",\"PG\",\"PB\",\"PK\"]#好\n",
    "    anger = [\"NA\"]#怒\n",
    "    sad = [\"NB\",\"NJ\",\"NH\",\"PF\"]#哀\n",
    "    afraid = [\"NI\",\"NC\",\"NG\"]#惧\n",
    "    boring = [\"NE\",\"ND\",\"NN\",\"NK\",\"NL\"]#恶\n",
    "    suprise = [\"PC\"]#惊\n",
    "    emotion = pd.read_excel('../data/weibo_data/senti onthology.xlsx', header=0, encoding='utf8')\n",
    "    words = emotion[\"词语\"].tolist()\n",
    "    emo = emotion[\"情感分类\"].tolist()\n",
    "    core = emotion[\"强度\"].tolist()\n",
    "    pos_neg = emotion[\"极性\"].tolist()\n",
    "    res = [i['W_content'] for i in result]\n",
    "\n",
    "    num_happy=0\n",
    "    num_like =0\n",
    "    num_anger=0\n",
    "    num_sad = 0\n",
    "    num_afraid = 0\n",
    "    num_boring = 0\n",
    "    num_suprise = 0\n",
    "    num_pos=0\n",
    "    num_neg=0\n",
    "    happy_content = []\n",
    "    like_content = []\n",
    "    anger_content = []\n",
    "    sad_content = []\n",
    "    afraid_content = []\n",
    "    boring_content = []\n",
    "    suprise_content = []\n",
    "    pos_content = []\n",
    "    neg_content = []\n",
    "    if len(res)>500:\n",
    "        N=500\n",
    "    else:\n",
    "        N = len(res)\n",
    "    for i in res[:N]: \n",
    "        sore_happy=0\n",
    "        sore_like =0\n",
    "        sore_anger=0\n",
    "        sore_sad = 0\n",
    "        sore_afraid = 0\n",
    "        sore_boring = 0\n",
    "        sore_suprise = 0\n",
    "        sore_pos=0\n",
    "        sore_neg=0\n",
    "        if len(i)>4:\n",
    "            for word in jieba.lcut(i):\n",
    "                if word in emotion[\"词语\"].tolist():\n",
    "                    aa = emotion.loc[emotion['词语']==word]\n",
    "                    if aa[\"情感分类\"].all() in happy:                  \n",
    "                         sore_happy= aa[\"强度\"].tolist()[0]+sore_happy\n",
    "                    elif aa[\"情感分类\"].all() in like:\n",
    "                        sore_like = aa[\"强度\"].tolist()[0]+sore_like\n",
    "                    elif aa[\"情感分类\"].all() in anger:\n",
    "                        sore_anger = aa[\"强度\"].tolist()[0]+sore_anger\n",
    "                    elif aa[\"情感分类\"].all() in sad:\n",
    "                        sore_sad = aa[\"强度\"].tolist()[0]+sore_sad\n",
    "                    elif aa[\"情感分类\"].all() in afraid:\n",
    "                        sore_afraid = aa[\"强度\"].tolist()[0]+sore_afraid\n",
    "                    elif aa[\"情感分类\"].all() in boring:\n",
    "                        sore_boring = aa[\"强度\"].tolist()[0]+sore_boring\n",
    "                    elif aa[\"情感分类\"].all() in suprise:\n",
    "                        sore_suprise = aa[\"强度\"].tolist()[0]+sore_suprise\n",
    "                    if aa[\"极性\"].tolist()[0] == 1:\n",
    "                        sore_pos = aa[\"强度\"].tolist()[0]+sore_pos\n",
    "                    elif aa[\"极性\"].tolist()[0] == 2:\n",
    "                        sore_neg = aa[\"强度\"].tolist()[0]+sore_neg\n",
    "            emo=[]\n",
    "            emo.append(sore_happy)\n",
    "            emo.append(sore_like)\n",
    "            emo.append(sore_anger)\n",
    "            emo.append(sore_sad)\n",
    "            emo.append(sore_afraid)\n",
    "            emo.append(sore_boring)\n",
    "            emo.append(sore_suprise)\n",
    "            senti = []\n",
    "            senti.append(sore_pos)\n",
    "            senti.append(sore_neg)\n",
    "            if max(emo) == sore_happy:\n",
    "                happy_content.append(i)\n",
    "                num_happy = num_happy+1\n",
    "            elif max(emo) == sore_like:\n",
    "                like_content.append(i)\n",
    "                num_like = num_like+1\n",
    "            elif max(emo) == sore_anger:\n",
    "                anger_content.append(i)\n",
    "                num_anger = num_anger+1\n",
    "            elif max(emo) == sore_sad:\n",
    "                sad_content.append(i)\n",
    "                num_sad = num_sad+1\n",
    "            elif max(emo) == sore_afraid:\n",
    "                afraid_content.append(i)\n",
    "                num_afraid = num_afraid+1\n",
    "            elif max(emo) == sore_boring:\n",
    "                boring_content.append(i)\n",
    "                num_boring = num_boring+1\n",
    "            elif max(emo) == sore_suprise:\n",
    "                suprise_content.append(i)\n",
    "                num_suprise = num_suprise+1\n",
    "            if max(senti) == sore_pos:\n",
    "                pos_content.append(i)\n",
    "                num_pos = num_pos+1\n",
    "            else:\n",
    "                neg_content.append(i)\n",
    "                num_neg = num_neg+1\n",
    "            sum1 = num_happy+num_like+num_anger+num_sad+num_afraid+num_boring+num_suprise\n",
    "            dict_emo = {\"happy\":round(num_happy/sum1,3),\"like\":round(num_like/sum1,3),\"anger\":round(num_anger/sum1,3),\n",
    "                        \"sad\":round(num_sad/sum1,3),\"afraid\":round(num_afraid/sum1,3),\"boring\":round(num_boring/sum1,3),\n",
    "                        \"suprise\":round(num_suprise/sum1,3)}\n",
    "            dict_senti = {\"positive\":round(num_pos/(num_pos+num_neg),3),\"negtive\":round(num_neg/(num_pos+num_neg),3)}\n",
    "            content_emo = {\"happy\":happy_content,\"like\":like_content,\"anger\":anger_content,\n",
    "                        \"sad\":sad_content,\"afraid\":afraid_content,\"boring\":boring_content,\n",
    "                        \"suprise\":suprise_content}\n",
    "            content_senti = {\"positive\":pos_content,\"negtive\":neg_content}\n",
    "    return dict_senti, pos_content[0:5], neg_content[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T11:49:42.245048Z",
     "start_time": "2019-07-24T11:49:42.238865Z"
    }
   },
   "outputs": [],
   "source": [
    "esObj = esWeiboTweetRetrieval(host='10.8.128.205',port=49200)\n",
    "\n",
    "docs = esObj.Retrieval(W_search_query='垃圾分类',W_content='垃圾分类',volume=100)\n",
    "\n",
    "dict_senti, pos_content, neg_content = get_weibo_senti(docs)\n",
    "\n",
    "result = {'dict_senti':dict_senti,\n",
    "         'pos_content':pos_content,\n",
    "         'neg_content':neg_content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negtive': 0.11, 'positive': 0.89}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#垃圾分类# 北京市法制宣传教育领导小组办公室、北京市司法局：“不分类 环境恶化、资源浪费”，“厨余垃圾→堆肥”。很想问问：现在北京厨余垃圾多少堆肥了？效率、效果、去向如何？能不能将混合垃圾和较纯厨余分开堆肥，对比一下？@北京市市政市容委@卫潘明@张红樱@立雯nu@垃圾战斗机FON@冯永锋',\n",
       " '#北京将立法约束垃圾分类#北京将立法明确垃圾分类个人责任，混合投放垃圾将被罚！#垃圾分类# 王希希ss的微博视频',\n",
       " '【北京拟对垃圾不分类的个人罚款，视频科普#干湿垃圾怎么区分# [思考]】日前，北京市城市管理委主任孙新军在做客节目时透露，《北京市生活垃圾管理条例》修订工作已列入2018-2020年的立法规划，新修订的条例拟对个人明确垃圾分类责任，混合投放垃圾最高要处以200元罚款，北京也不会低于这个数。对于北京针对垃圾分类的骤然提速，很多网友非常懵：各类垃圾到底怎么分？#垃圾分类# 视频科普：http://t.cn/AipS8vSa',\n",
       " '#你是个什么垃圾北京版# 一个#垃圾分类# 连续上热门得有一周了吧，还以为上海人民先替全国人民受难会坚持一年半载，没想到全国推行这么快，北京也要实施了[允悲] 关键是北京、上海标准还不一样，像我们这种一年有四个月在上海出差的人，要背两套规定啊[允悲][允悲][允悲][允悲]  #北京将推动垃圾分类立法#',\n",
       " '#垃圾分类# 上海北京步入垃圾分类强制时代，来自河北的你做好准备了吗？ [组图共2张]']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['【[话筒]提前转存！北京将推动垃圾分类立法 分类标准与上海不完全一致】#垃圾分类# 日前，记者从北京市城管委了解到，北京将推动垃圾分类立法，罚款不低于上海。但北京的垃圾分类标准与上海并不完全一致，分为厨余垃圾、有害垃圾、其它垃圾及可回收物。具体怎么分？戳视频↓  南昌高新区发布的微博视频',\n",
       " '来自网络评论：《北京青年》就是一个北京老年用一群北京中年来演绎空想的北京青年的故事。妙哉！',\n",
       " '北京政府真是无耻到了极点，估计过几年来北京就得签证了！！！！！',\n",
       " '#垃圾分类 卡路里#闻圾起舞寓教于乐[给力] 北京人民也在瑟瑟发抖[允悲][允悲][允悲]谈小培在旅途的秒拍视频',\n",
       " 'http://t.cn/Ai0SCtdm 2017年北京生活垃圾清运量为924.77万吨，如果用载重5吨的卡车来运送，两辆一排，可以从云南西双版纳排到哈尔滨。目前我国城市垃圾处理最主要的方式是填埋与焚烧，但都存在一定的弊端，假设北京的生活#垃圾分类# 妥善，每年因垃圾致癌的人数将从241人降低至182人。@第一财经日报 [组图共5张]']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
