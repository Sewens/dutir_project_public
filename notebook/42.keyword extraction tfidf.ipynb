{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_pickle(obj,fname):\n",
    "    with open(fname,'wb') as file:\n",
    "        pickle.dump(obj,file)\n",
    "def load_pickle(fname):\n",
    "    return pickle.load(open(fname,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_df = pd.read_pickle('../data/patent_data/processed/claim_df_seg_0630.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_text = claim_df.seg_text.tolist()\n",
    "patent_text = [' '.join(i) for i in patent_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量化 使用tfidf 重新训练 限定词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_tfidf_obj = TfidfVectorizer(max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_tfidf_obj.fit(patent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(patent_tfidf_obj,'../data/patent_data/processed/patent_tfidf_obj_1wdim_0701.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patent_all_tfidf_matrix = patent_tfidf_obj.transform(patent_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 样例数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_patent_data_tfidf = patent_tfidf_obj.transform(patent_text[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚类 使用 k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关键词抽取做辅助"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.773 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from textrank4zh import TextRank4Keyword,TextRank4Sentence\n",
    "\n",
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(patent_text[0],lower=True,window=2)\n",
    "\n",
    "def topkword(text,topk=20):\n",
    "    tr4w.analyze(text,lower=True,window=2)\n",
    "    return tr4w.keywords[:topk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf 抽取关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topkword_tfidf(text,topk=20,tfidf_obj=patent_tfidf_obj):\n",
    "    '''\n",
    "    输入text为空格分隔的词们\n",
    "    '''\n",
    "    tfidf_vec = patent_tfidf_obj.transform([text])\n",
    "    tfidf_vec = tfidf_vec.toarray()[0]\n",
    "    wordlist = list(patent_tfidf_obj.vocabulary_.keys())\n",
    "    order = np.argsort(tfidf_vec)[::-1]\n",
    "    kwd = [wordlist[k] for k in order]\n",
    "    pattern_none_chinese = re.compile('[0-9a-zA-Z]')\n",
    "    kwd = list(filter(lambda x:not re.match(pattern_none_chinese,x),kwd))\n",
    "    return kwd[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一种 处理器 ， 包括 ： 位于 单个 芯片 封装 内 的 多个 处理 核 ， 其中 每个 处理 核 进一步 包括 ： 指令 取出 逻辑 单元 ， 用于 取出 一个 或 多个 指令 ； 指令 解码 逻辑 单元 ， 用于 解码 一个 或 多个 指令 ； 寄存器 文件 ， 包括 一组 向量 寄存器 ， 每个 向量 寄存器 存储 多个 向量 数据 元素 ； 执行 单元 ， 用于 执行 第一 指令 以 读取 单精度 浮点 值 ， 将 该值 转换 为 双 精度 浮点 值 ， 以及 将 结果 作为 可 变数 目的 N 个 向量 数据 元素 存储 在 寄存器 文件 的 向量 寄存器 中 ， 其中 所述 可变 数目 响应 于 所 执行 的 第一 指令 为 一个 或 多个 ； 其中 所述 执行 单元 用于 执行 第二 指令 以 将 单精度 浮点 值 转换 为 带符号 整数 值 并 将 结果 存储 在 储存 位置 ， 并且 其中 所述 执行 单元 用于 执行 第三 指令 以 将 单精度 浮点 值 转换 为 无 符号 整数 值 并 将 结果 存储 在 储存 位置 。 如权利要求 1 所述 的 处理器 ， 其特征在于 ， 所述 寄存器 文件 包括 一组 物理 寄存器 ， 用于 存储 浮点 值 和 向量 数据 元素 。'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['脱硫',\n",
       " '速比',\n",
       " '耦合',\n",
       " '近似',\n",
       " '那个',\n",
       " '络合物',\n",
       " '照相机',\n",
       " '偏置',\n",
       " '倾斜地',\n",
       " '端面',\n",
       " '基站',\n",
       " '图像编码',\n",
       " '波束',\n",
       " '灭火',\n",
       " '图像处理',\n",
       " '炎症',\n",
       " '圆环',\n",
       " '绝缘体',\n",
       " '收纳',\n",
       " '还原性']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topkword_tfidf(patent_text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tue Jul  2 09:47:47 2019] Num:0\n",
      "[Tue Jul  2 09:48:03 2019] Num:1000\n",
      "[Tue Jul  2 09:48:18 2019] Num:2000\n",
      "[Tue Jul  2 09:48:33 2019] Num:3000\n",
      "[Tue Jul  2 09:48:48 2019] Num:4000\n",
      "[Tue Jul  2 09:49:03 2019] Num:5000\n",
      "[Tue Jul  2 09:49:19 2019] Num:6000\n",
      "[Tue Jul  2 09:49:34 2019] Num:7000\n",
      "[Tue Jul  2 09:49:49 2019] Num:8000\n",
      "[Tue Jul  2 09:50:04 2019] Num:9000\n",
      "[Tue Jul  2 09:50:19 2019] Num:10000\n",
      "[Tue Jul  2 09:50:35 2019] Num:11000\n",
      "[Tue Jul  2 09:50:50 2019] Num:12000\n",
      "[Tue Jul  2 09:51:05 2019] Num:13000\n",
      "[Tue Jul  2 09:51:20 2019] Num:14000\n",
      "[Tue Jul  2 09:51:35 2019] Num:15000\n",
      "[Tue Jul  2 09:51:50 2019] Num:16000\n",
      "[Tue Jul  2 09:52:06 2019] Num:17000\n",
      "[Tue Jul  2 09:52:21 2019] Num:18000\n",
      "[Tue Jul  2 09:52:36 2019] Num:19000\n",
      "[Tue Jul  2 09:52:52 2019] Num:20000\n",
      "[Tue Jul  2 09:53:07 2019] Num:21000\n",
      "[Tue Jul  2 09:53:22 2019] Num:22000\n",
      "[Tue Jul  2 09:53:37 2019] Num:23000\n",
      "[Tue Jul  2 09:53:52 2019] Num:24000\n",
      "[Tue Jul  2 09:54:07 2019] Num:25000\n",
      "[Tue Jul  2 09:54:23 2019] Num:26000\n",
      "[Tue Jul  2 09:54:38 2019] Num:27000\n",
      "[Tue Jul  2 09:54:53 2019] Num:28000\n",
      "[Tue Jul  2 09:55:08 2019] Num:29000\n",
      "[Tue Jul  2 09:55:23 2019] Num:30000\n",
      "[Tue Jul  2 09:55:38 2019] Num:31000\n",
      "[Tue Jul  2 09:55:54 2019] Num:32000\n",
      "[Tue Jul  2 09:56:09 2019] Num:33000\n",
      "[Tue Jul  2 09:56:24 2019] Num:34000\n",
      "[Tue Jul  2 09:56:39 2019] Num:35000\n",
      "[Tue Jul  2 09:56:54 2019] Num:36000\n",
      "[Tue Jul  2 09:57:09 2019] Num:37000\n",
      "[Tue Jul  2 09:57:24 2019] Num:38000\n",
      "[Tue Jul  2 09:57:40 2019] Num:39000\n",
      "[Tue Jul  2 09:57:55 2019] Num:40000\n",
      "[Tue Jul  2 09:58:10 2019] Num:41000\n",
      "[Tue Jul  2 09:58:25 2019] Num:42000\n",
      "[Tue Jul  2 09:58:40 2019] Num:43000\n",
      "[Tue Jul  2 09:58:56 2019] Num:44000\n",
      "[Tue Jul  2 09:59:11 2019] Num:45000\n",
      "[Tue Jul  2 09:59:26 2019] Num:46000\n",
      "[Tue Jul  2 09:59:41 2019] Num:47000\n",
      "[Tue Jul  2 09:59:56 2019] Num:48000\n",
      "[Tue Jul  2 10:00:11 2019] Num:49000\n",
      "[Tue Jul  2 10:00:26 2019] Num:50000\n",
      "[Tue Jul  2 10:00:42 2019] Num:51000\n",
      "[Tue Jul  2 10:00:57 2019] Num:52000\n",
      "[Tue Jul  2 10:01:12 2019] Num:53000\n",
      "[Tue Jul  2 10:01:27 2019] Num:54000\n",
      "[Tue Jul  2 10:01:42 2019] Num:55000\n",
      "[Tue Jul  2 10:01:57 2019] Num:56000\n",
      "[Tue Jul  2 10:02:13 2019] Num:57000\n",
      "[Tue Jul  2 10:02:28 2019] Num:58000\n",
      "[Tue Jul  2 10:02:43 2019] Num:59000\n",
      "[Tue Jul  2 10:02:58 2019] Num:60000\n",
      "[Tue Jul  2 10:03:13 2019] Num:61000\n",
      "[Tue Jul  2 10:03:29 2019] Num:62000\n",
      "[Tue Jul  2 10:03:44 2019] Num:63000\n",
      "[Tue Jul  2 10:03:59 2019] Num:64000\n",
      "[Tue Jul  2 10:04:14 2019] Num:65000\n",
      "[Tue Jul  2 10:04:30 2019] Num:66000\n",
      "[Tue Jul  2 10:04:45 2019] Num:67000\n",
      "[Tue Jul  2 10:05:00 2019] Num:68000\n",
      "[Tue Jul  2 10:05:15 2019] Num:69000\n",
      "[Tue Jul  2 10:05:30 2019] Num:70000\n",
      "[Tue Jul  2 10:05:45 2019] Num:71000\n",
      "[Tue Jul  2 10:06:01 2019] Num:72000\n",
      "[Tue Jul  2 10:06:16 2019] Num:73000\n",
      "[Tue Jul  2 10:06:31 2019] Num:74000\n",
      "[Tue Jul  2 10:06:46 2019] Num:75000\n",
      "[Tue Jul  2 10:07:01 2019] Num:76000\n",
      "[Tue Jul  2 10:07:17 2019] Num:77000\n",
      "[Tue Jul  2 10:07:32 2019] Num:78000\n",
      "[Tue Jul  2 10:07:47 2019] Num:79000\n",
      "[Tue Jul  2 10:08:02 2019] Num:80000\n",
      "[Tue Jul  2 10:08:17 2019] Num:81000\n",
      "[Tue Jul  2 10:08:32 2019] Num:82000\n",
      "[Tue Jul  2 10:08:48 2019] Num:83000\n",
      "[Tue Jul  2 10:09:03 2019] Num:84000\n",
      "[Tue Jul  2 10:09:18 2019] Num:85000\n",
      "[Tue Jul  2 10:09:33 2019] Num:86000\n",
      "[Tue Jul  2 10:09:48 2019] Num:87000\n",
      "[Tue Jul  2 10:10:04 2019] Num:88000\n",
      "[Tue Jul  2 10:10:19 2019] Num:89000\n",
      "[Tue Jul  2 10:10:34 2019] Num:90000\n",
      "[Tue Jul  2 10:10:49 2019] Num:91000\n",
      "[Tue Jul  2 10:11:04 2019] Num:92000\n",
      "[Tue Jul  2 10:11:19 2019] Num:93000\n",
      "[Tue Jul  2 10:11:34 2019] Num:94000\n",
      "[Tue Jul  2 10:11:50 2019] Num:95000\n",
      "[Tue Jul  2 10:12:05 2019] Num:96000\n",
      "[Tue Jul  2 10:12:20 2019] Num:97000\n",
      "[Tue Jul  2 10:12:35 2019] Num:98000\n",
      "[Tue Jul  2 10:12:50 2019] Num:99000\n",
      "[Tue Jul  2 10:13:05 2019] Num:100000\n",
      "[Tue Jul  2 10:13:21 2019] Num:101000\n",
      "[Tue Jul  2 10:13:36 2019] Num:102000\n",
      "[Tue Jul  2 10:13:51 2019] Num:103000\n",
      "[Tue Jul  2 10:14:06 2019] Num:104000\n",
      "[Tue Jul  2 10:14:21 2019] Num:105000\n",
      "[Tue Jul  2 10:14:36 2019] Num:106000\n",
      "[Tue Jul  2 10:14:52 2019] Num:107000\n",
      "[Tue Jul  2 10:15:07 2019] Num:108000\n",
      "[Tue Jul  2 10:15:22 2019] Num:109000\n",
      "[Tue Jul  2 10:15:37 2019] Num:110000\n",
      "[Tue Jul  2 10:15:52 2019] Num:111000\n",
      "[Tue Jul  2 10:16:08 2019] Num:112000\n",
      "[Tue Jul  2 10:16:23 2019] Num:113000\n",
      "[Tue Jul  2 10:16:38 2019] Num:114000\n",
      "[Tue Jul  2 10:16:53 2019] Num:115000\n",
      "[Tue Jul  2 10:17:08 2019] Num:116000\n",
      "[Tue Jul  2 10:17:24 2019] Num:117000\n",
      "[Tue Jul  2 10:17:39 2019] Num:118000\n",
      "[Tue Jul  2 10:17:54 2019] Num:119000\n",
      "[Tue Jul  2 10:18:09 2019] Num:120000\n",
      "[Tue Jul  2 10:18:24 2019] Num:121000\n",
      "[Tue Jul  2 10:18:40 2019] Num:122000\n",
      "[Tue Jul  2 10:18:55 2019] Num:123000\n",
      "[Tue Jul  2 10:19:10 2019] Num:124000\n",
      "[Tue Jul  2 10:19:25 2019] Num:125000\n",
      "[Tue Jul  2 10:19:40 2019] Num:126000\n",
      "[Tue Jul  2 10:19:55 2019] Num:127000\n",
      "[Tue Jul  2 10:20:10 2019] Num:128000\n",
      "[Tue Jul  2 10:20:25 2019] Num:129000\n",
      "[Tue Jul  2 10:20:40 2019] Num:130000\n",
      "[Tue Jul  2 10:20:55 2019] Num:131000\n",
      "[Tue Jul  2 10:21:11 2019] Num:132000\n",
      "[Tue Jul  2 10:21:26 2019] Num:133000\n",
      "[Tue Jul  2 10:21:41 2019] Num:134000\n",
      "[Tue Jul  2 10:21:56 2019] Num:135000\n",
      "[Tue Jul  2 10:22:11 2019] Num:136000\n",
      "[Tue Jul  2 10:22:26 2019] Num:137000\n",
      "[Tue Jul  2 10:22:41 2019] Num:138000\n",
      "[Tue Jul  2 10:22:56 2019] Num:139000\n",
      "[Tue Jul  2 10:23:12 2019] Num:140000\n",
      "[Tue Jul  2 10:23:27 2019] Num:141000\n",
      "[Tue Jul  2 10:23:42 2019] Num:142000\n",
      "[Tue Jul  2 10:23:57 2019] Num:143000\n",
      "[Tue Jul  2 10:24:12 2019] Num:144000\n",
      "[Tue Jul  2 10:24:27 2019] Num:145000\n"
     ]
    }
   ],
   "source": [
    "with open('../data/patent_data/processed/claim_kword_tfidf.txt','w',encoding='utf-8') as file:\n",
    "    for num,_ in enumerate(claim_df.itertuples()):\n",
    "        if num % 1000==0:\n",
    "            print('[{0}] Num:{1}'.format(time.ctime(),num))\n",
    "        kw = topkword_tfidf(' '.join(claim_df.loc[num,'seg_text']))\n",
    "        kw = '|'.join(kw)\n",
    "        file.write(kw+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
