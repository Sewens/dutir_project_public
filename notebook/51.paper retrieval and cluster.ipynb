{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将检索模块和文本聚类模块进行封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T13:21:34.849399Z",
     "start_time": "2019-07-12T13:21:34.314684Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES检索模块重构为一个对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T13:21:34.880361Z",
     "start_time": "2019-07-12T13:21:34.854063Z"
    }
   },
   "outputs": [],
   "source": [
    "class esPaperRetrieval():\n",
    "    '''\n",
    "    根据论文检索需求进行功能的微调\n",
    "    '''\n",
    "    _instance = None\n",
    "    _first_init = True\n",
    "    def __new__(cls, *args, **kw):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(esPatentRetrieval, cls).__new__(cls)  \n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, host, port):\n",
    "        '''\n",
    "        使用ES进行论文检索 指定host、port以及专利index之后进行检索\n",
    "        '''\n",
    "        super(esPatentRetrieval, self).__init__()\n",
    "        self.es = Elasticsearch(hosts=host, port=port, timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "        self.indexName = 'patent-index'\n",
    "\n",
    "    def do_search(self, query, fromDate, toDate, volume):\n",
    "        '''\n",
    "        do_search方法执行具体检索过程\n",
    "        query传入检索词 formDate和toDate分别为检索的前后时间\n",
    "        volume为返回文档数目\n",
    "        '''\n",
    "        queryKeywordDict = {\"match\": {\"text\": query}\n",
    "                            }\n",
    "        queryDateRangeDict = {\"range\": {\n",
    "            \"publicationDate\": {\n",
    "                \"gt\": fromDate,\n",
    "                \"lt\": toDate\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "        queryBody = {\"query\": {\"bool\": {\n",
    "            \"must\": [queryDateRangeDict, queryKeywordDict],\n",
    "            \"must_not\": [],\n",
    "            \"should\": []\n",
    "        }},\n",
    "            \"from\": 0,\n",
    "            \"size\": volume,\n",
    "            \"sort\": [],\n",
    "            \"aggs\": {}\n",
    "        }\n",
    "        result = self.es.search(index=self.indexName, body=queryBody)\n",
    "        return result\n",
    "\n",
    "    def format_search(self, result):\n",
    "        '''\n",
    "        format_search方法对检索结果进行格式化 构建符合要求的字段进行返回\n",
    "        输入result为检索结果 提取其中的检索结果进行后处理\n",
    "        使用ES检索后得到的结果中result['hits']['hits']为数组格式数据\n",
    "        其中每一个元素为一个dict 对应部分字段\n",
    "        '''\n",
    "        docs = result['hits']['hits']\n",
    "        docs = [i['_source'] for i in docs]\n",
    "        targetKeyList = 'id pub_id abstractDesc text examiner priorityDocNum agentName title applicationDocNum agentPersonName applicant ipcMain priorityDate inventor assignee publicationDate ipcList applicationDate firstApplicant'\n",
    "        targetKeyList = targetKeyList.split(' ')\n",
    "        dict_filter_by_keys = lambda d: {k: d[k] for k in targetKeyList}\n",
    "        dict_filter_text = lambda d: {k if not k == 'text' else 'claim_text': d[k] for k in d}\n",
    "        dict_filter_id = lambda d: {k if not k == '_id' else 'id': d[k] for k in d}\n",
    "        docs = (dict_filter_by_keys(doc) for doc in docs)\n",
    "        docs = [dict_filter_text(doc) for doc in docs]\n",
    "        return docs\n",
    "    def Retrieval(self, query, fromDate, toDate, volume):\n",
    "        result = self.do_search(query, fromDate, toDate, volume)\n",
    "        docs = self.format_search(result)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T13:21:37.478618Z",
     "start_time": "2019-07-12T13:21:37.473064Z"
    }
   },
   "outputs": [],
   "source": [
    "es_obj = esPatentRetrieval(host='10.8.128.205',port=29200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T13:21:41.874387Z",
     "start_time": "2019-07-12T13:21:39.106005Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = es_obj.Retrieval('机器人','2000-01-06','2010-01-06',volume=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 客户端对于后端数据接口的访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T08:51:22.192286Z",
     "start_time": "2019-07-12T08:51:18.717377Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "body = {'searchQuery':'机器人',\n",
    "        'fromDate':'2000-06-02',\n",
    "        'toDate':'2016-01-01',\n",
    "        'volume':1000}\n",
    "\n",
    "baseUrl = 'http://10.8.128.205:29280/Lawbda/dataWare/1.0.0/patent/search'\n",
    "\n",
    "docs = requests.get(baseUrl,params=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本聚类过程封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T13:22:01.750858Z",
     "start_time": "2019-07-12T13:22:01.110829Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T13:22:01.776486Z",
     "start_time": "2019-07-12T13:22:01.753761Z"
    }
   },
   "outputs": [],
   "source": [
    "class topicCluster():\n",
    "    _instance = None\n",
    "    _first_init = True\n",
    "    def __new__(cls, *args, **kw):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(topicCluster, cls).__new__(cls)\n",
    "        return cls._instance\n",
    "    def __init__(self,):\n",
    "        # 初始化分词器\n",
    "        if self._first_init:\n",
    "            print('init jieba')\n",
    "            self.tokenizer = lambda x:jieba.lcut(x)\n",
    "            self.tokenizer('我爱北京天安门')\n",
    "            # 读取向量化对象\n",
    "            print('init vectirization')\n",
    "            self.tfidf_obj = self.load_pickle('../data/patent_data/model/patent_tfidf_model.pkl')\n",
    "            self.lda_obj = self.load_pickle('../data/patent_data/model/patent_lda_topic9_model.pkl')\n",
    "            # 读取关键词列表\n",
    "            print('init kword')\n",
    "            self.keyword_df = pd.read_pickle('../data/patent_data/processed/claim_kword_df_tr4w.pkl')\n",
    "            kword_map = {i[2]:i[4] for i in self.keyword_df.itertuples()}\n",
    "            self.kword_map = {k:kword_map[k].split(' ') for k in kword_map}\n",
    "            print('done!')\n",
    "            self._first_init = False\n",
    "    def data_wash(self,_str):\n",
    "        '''\n",
    "        数据清洗对象\n",
    "        '''\n",
    "        pattern_num = re.compile('\\d{4}\\.')\n",
    "        pattern_num1 = re.compile('\\d{1}\\.')\n",
    "        _str = re.sub(pattern_num,'',_str)\n",
    "        _str = re.sub(pattern_num1,'',_str)\n",
    "        return _str\n",
    "    \n",
    "    def do_cluster(self,texts,n_clusters):\n",
    "        '''\n",
    "        文本聚类对象\n",
    "        '''\n",
    "        km_obj = KMeans(n_clusters=n_clusters)\n",
    "        texts = [self.data_wash(i) for i in texts]\n",
    "        texts = [self.tokenizer(i) for i in texts]\n",
    "        texts = [' '.join(i) for i in texts]\n",
    "        tfidf_vec = self.tfidf_obj.transform(texts)\n",
    "        lda_vec = self.lda_obj.transform(tfidf_vec)\n",
    "        cluster_result = km_obj.fit_predict(lda_vec)\n",
    "        return cluster_result\n",
    "    \n",
    "    def do_kword(self,ids):\n",
    "        '''\n",
    "        由于预先对文本进行关键词抽取 因此只要传入对应文本id即可查表找到关键词\n",
    "        '''\n",
    "        kword_list = [self.kword_map[k] for k in ids]\n",
    "        return kword_list\n",
    "    \n",
    "    def do_format(self,cluster_wordtable):\n",
    "        '''\n",
    "        用于将输出结果格式化为API要求格式\n",
    "        '''\n",
    "        api_result = []\n",
    "        for topicID in cluster_wordtable:\n",
    "            temp_dict = {}\n",
    "            temp_dict['topicID'] = topicID\n",
    "            temp_dict['topicWords'] = []\n",
    "            for k in cluster_wordtable[topicID]:\n",
    "                temp_temp_dict = {}\n",
    "                temp_temp_dict['word'] = k\n",
    "                temp_temp_dict['freq'] = float(cluster_wordtable[topicID][k])\n",
    "                temp_dict['topicWords'].append(temp_temp_dict)\n",
    "            api_result.append(temp_dict)\n",
    "        return api_result\n",
    "    def Cluster(self,ids,texts,n_clusters):\n",
    "        '''\n",
    "        主方法 传入文本以及文本对应id \n",
    "        输出聚类后文本类别和对应关键词\n",
    "        如要求聚类数目为10 则输出\n",
    "        '''\n",
    "        cluster = self.do_cluster(texts,n_clusters)\n",
    "        kword = self.do_kword(ids)\n",
    "        topk = 20\n",
    "        cluster_wordtable = {}\n",
    "        for num,i in enumerate(cluster):\n",
    "            if not i in cluster_wordtable:\n",
    "                cluster_wordtable[i] = {}\n",
    "            kword_list = kword[num]\n",
    "            for word in kword_list:\n",
    "                if not word in cluster_wordtable[i]:\n",
    "                    cluster_wordtable[i][word] = 0\n",
    "                cluster_wordtable[i][word]+=1\n",
    "            cluster_wordtable[i] = {k:cluster_wordtable[i][k]for k in \n",
    "                                    sorted(cluster_wordtable[i],key=lambda x:cluster_wordtable[i][x],reverse=True)[:topk]}\n",
    "        final_result = self.do_format(cluster_wordtable)\n",
    "        return final_result\n",
    "    @staticmethod\n",
    "    def dump_pickle(obj,fname):\n",
    "        with open(fname,'wb') as file:\n",
    "            pickle.dump(obj,file)\n",
    "    @staticmethod\n",
    "    def load_pickle(fname):\n",
    "        return pickle.loads(open(fname,'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T13:22:07.442888Z",
     "start_time": "2019-07-12T13:22:02.082034Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init jieba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.793 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init vectirization\n",
      "init kword\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "ppp = topicCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T13:22:07.448463Z",
     "start_time": "2019-07-12T13:22:07.445539Z"
    }
   },
   "outputs": [],
   "source": [
    "qqq = topicCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T13:22:07.544980Z",
     "start_time": "2019-07-12T13:22:07.450184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppp == qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T12:31:40.051988Z",
     "start_time": "2019-07-10T12:31:39.720606Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [i['claim_text'] for i in docs.json()]\n",
    "ids = [i['pub_id'] for i in docs.json()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T12:39:15.601461Z",
     "start_time": "2019-07-10T12:39:07.927135Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster = ppp.Cluster(ids,texts,n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T08:43:07.172900Z",
     "start_time": "2019-07-12T08:43:07.166023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qqq == ppp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时间测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T08:51:27.015676Z",
     "start_time": "2019-07-12T08:51:27.011377Z"
    }
   },
   "outputs": [],
   "source": [
    "fromDate = '2010-01-01'\n",
    "toDate = '2016-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T08:56:14.523688Z",
     "start_time": "2019-07-12T08:56:14.519105Z"
    }
   },
   "outputs": [],
   "source": [
    "ppp = RetrievalMeta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T08:55:17.539715Z",
     "start_time": "2019-07-12T08:55:17.519246Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'Retrieval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-66a00c7f8fd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mesPatentSearchObject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mesPatentRetrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'10.8.128.205'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m29200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mesPatentSearchObject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearchQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromDate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfromDate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoDate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoDate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'claim_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pub_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcluster_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopicCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'Retrieval'"
     ]
    }
   ],
   "source": [
    "esPatentSearchObject = esPatentRetrieval(host='10.8.128.205', port=29200)\n",
    "docs = esPatentSearchObject.Retrieval(query=searchQuery, fromDate=fromDate, toDate=toDate, volume=5000)\n",
    "texts = [i['claim_text'] for i in docs]\n",
    "ids = [i['pub_id'] for i in docs]\n",
    "cluster_obj = topicCluster()\n",
    "result = cluster_obj.Cluster(ids=ids, texts=texts, n_clusters=topicNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esPatentSearchObject = esPatentRetrieval(host='10.8.128.205', port=29200)\n",
    "docs = esPatentSearchObject.Retrieval(query=searchQuery, fromDate=fromDate, toDate=toDate, volume=5000)\n",
    "texts = [i['claim_text'] for i in docs]\n",
    "ids = [i['pub_id'] for i in docs]\n",
    "cluster_obj = topicCluster()\n",
    "result = cluster_obj.Cluster(ids=ids, texts=texts, n_clusters=topicNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时序聚类过程的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:09:48.377303Z",
     "start_time": "2019-07-10T13:09:48.370569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:10:43.119277Z",
     "start_time": "2019-07-10T13:10:43.115401Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:10:49.546739Z",
     "start_time": "2019-07-10T13:10:49.540792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
