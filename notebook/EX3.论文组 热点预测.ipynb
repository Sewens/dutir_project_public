{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-21T05:49:50.095Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import jieba\n",
    "import re,time\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#导入PCA算法库\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence\n",
    "from sklearn import ensemble\n",
    "from functools import reduce\n",
    "from sklearn.metrics import mean_squared_error #均方误差\n",
    "from sklearn.metrics import mean_absolute_error #平方绝对误差\n",
    "from sklearn.metrics import r2_score#R square\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T05:45:51.170896Z",
     "start_time": "2019-07-21T05:45:50.918145Z"
    }
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T05:45:51.195878Z",
     "start_time": "2019-07-21T05:45:51.173472Z"
    }
   },
   "outputs": [],
   "source": [
    "def distance(a, b):\n",
    "    #  计算两个vector的距离，逐项差的平方和，不开根号\n",
    "    if len(a) != len(b):\n",
    "        print('wrong')\n",
    "        return 0\n",
    "    else:\n",
    "        temp = a - b\n",
    "        temp = temp ** 2\n",
    "        res = sum(temp)\n",
    "        return res\n",
    "def get_dist_mat(feat):\n",
    "    #  输入为特征矩阵，返回一个相似度矩阵（行数乘行数）\n",
    "    print(type(feat))\n",
    "    length=feat.shape[0]#样本数量\n",
    "    wide = feat.shape[1]#特征向量长度\n",
    "    res = np.zeros(shape=(length, length))\n",
    "    ave = np.zeros(shape=(length, length, wide))\n",
    "\n",
    "    for i in range(0, length):\n",
    "        for j in range(i+1, length):\n",
    "            ave[i, j] = np.mean(feat[i: j+1], axis=0)\n",
    "            # print('i = %d, j = %d' % (i, j))\n",
    "\n",
    "    for i in range(0, length):\n",
    "        for j in range(i+1, length):\n",
    "            ave_one = ave[i, j]\n",
    "            dist_lst = [distance(ave_one, one) for one in feat[i: j+1]]\n",
    "            res[i, j] = sum(dist_lst)\n",
    "            # print('i = %d, j = %d' % (i, j))\n",
    "    return res\n",
    "\n",
    "def get_class_devide(dist_mat, class_num):\n",
    "    #  利用递推公式计算分成class_num类别的分割方法\n",
    "    length = len(dist_mat)\n",
    "    divided_point = np.zeros(shape=(length, class_num+1))  # 存切割点\n",
    "    diveded_dist = np.zeros(shape=(length, class_num+1))  # 存切割最小距离\n",
    "\n",
    "    for i in range(1, length):  # 分成两类\n",
    "        dist_lst = [dist_mat[0, k] + dist_mat[k+1, i] for k in range(0, i)]\n",
    "        divided_point[i, 2] = np.argmin(dist_lst)\n",
    "        diveded_dist[i, 2] = np.min(dist_lst)\n",
    "\n",
    "    for classes in range(3, class_num+1):   # 分成多类\n",
    "        for i in range(classes-1, length):\n",
    "            dist_lst = [diveded_dist[k, classes-1] + dist_mat[k+1, i] for k in range(classes-2, i)]  # 从n-1类到n类\n",
    "            divided_point[i, classes] = np.argmin(dist_lst) + classes - 2\n",
    "            diveded_dist[i, classes] = np.min(dist_lst)\n",
    "    return diveded_dist, divided_point\n",
    "\n",
    "def get_trace(trace_mat, class_num):\n",
    "    #  获得切割方法list，\n",
    "    #  trace_mat就是get_class_devide函数返回的divided_point，分割点记录矩阵\n",
    "    lst = []\n",
    "    length = len(trace_mat)  #length=38\n",
    "    pre = length - 1   #pre=37\n",
    "    for i in range(class_num, 1, -1):\n",
    "        temp = int(trace_mat[pre, i])\n",
    "        pre = temp\n",
    "        lst.append(temp)\n",
    "    lst.reverse()\n",
    "    return lst\n",
    "def get_one_essay_trace(feature,phrase_num):\n",
    "    #  整合调用 有序聚类方法\n",
    "    #  调用就可以获得一个文件的分割结果并存入本地\n",
    "\n",
    "    time_start = time.time()\n",
    "    dist_mat = get_dist_mat(feature)\n",
    "    phrase_num = phrase_num\n",
    "    min_dist_mat, min_trace_mat = get_class_devide(dist_mat, phrase_num)\n",
    "    # print(min_trace_mat)\n",
    "\n",
    "    lst = get_trace(min_trace_mat, phrase_num)\n",
    "    lst.append(len(feature))\n",
    "    print('划分结果为：',end='')\n",
    "    print(lst)\n",
    "    time_end = time.time()\n",
    "    print('cost time', time_end - time_start)\n",
    "    return lst,min_dist_mat,min_trace_mat\n",
    "def observe_divided(lst, sents):\n",
    "    # 展示分割结果\n",
    "    split_result=[]\n",
    "    start = 0\n",
    "    for end in lst:\n",
    "        cla_lst=[]\n",
    "        if end==len(sents):\n",
    "            end=end-1\n",
    "        for i in range(start, end+1):\n",
    "            cla_lst.append(sents[i])\n",
    "        start = end + 1\n",
    "        split_result.append(cla_lst)\n",
    "    return split_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T05:45:51.387515Z",
     "start_time": "2019-07-21T05:45:51.198702Z"
    }
   },
   "outputs": [],
   "source": [
    "class esPaperRetrieval():\n",
    "    '''\n",
    "    根据论文检索需求进行功能的微调\n",
    "    '''\n",
    "    _instance = None\n",
    "    _first_init = True\n",
    "    def __new__(cls, *args, **kw):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(esPaperRetrieval, cls).__new__(cls)  \n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, host, port):\n",
    "        '''\n",
    "        使用ES进行论文检索 指定host、port以及专利index之后进行检索\n",
    "        '''\n",
    "        super(esPaperRetrieval, self).__init__()\n",
    "        self.es = Elasticsearch(hosts=host, port=port, timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "        self.indexName = 'paper-detail-index'\n",
    "\n",
    "    def do_search(self, titleQuery, kwordQuery, summaryQuery ,pubQuery,fromDate, toDate, volume):\n",
    "        '''\n",
    "        do_search方法执行具体检索过程\n",
    "        titleQuery 为对应标题检索词\n",
    "        kwordQuery 为对应关键字检索词\n",
    "        summaryQuery 为对应摘要检索词\n",
    "        上述三个词为or模式 可以出现 可以不出现\n",
    "        pubQuery 为对应文摘关键词 这个关键词必定匹配\n",
    "        fromDate 为检索字段起始日期 toDate为终结日期 日期格式 yyyy-mm-dd\n",
    "        volume为每次检索返回的数目\n",
    "        '''\n",
    "        queryBody = {\n",
    "          \"query\": {\n",
    "            \"bool\": {\n",
    "              \"should\": [\n",
    "                {\n",
    "                  \"term\": {\n",
    "                    \"P_Title\": titleQuery\n",
    "                  }\n",
    "                },\n",
    "                {\n",
    "                  \"term\": {\n",
    "                    \"P_Keyword\": kwordQuery\n",
    "                  }\n",
    "                },\n",
    "                {\n",
    "                  \"term\": {\n",
    "                    \"P_Summary\": summaryQuery\n",
    "                  }\n",
    "                },\n",
    "                  {\n",
    "                  \"term\": {\n",
    "                    \"P_Publication.keyword\": {\"value\":pubQuery}\n",
    "                  }\n",
    "                  }\n",
    "              ],\n",
    "              \"filter\": [\n",
    "                {\n",
    "                  \"range\": {\n",
    "                    \"P_year\": {\n",
    "                      \"gt\": fromDate,\n",
    "                      \"lt\": toDate\n",
    "                    }\n",
    "                  }\n",
    "                },\n",
    "              ]\n",
    "            }\n",
    "          },\n",
    "            \"from\": 0,\n",
    "            \"size\": volume,\n",
    "            \"sort\": [],\n",
    "            \"aggs\": {}\n",
    "        }\n",
    "#         print(queryBody)\n",
    "        result = self.es.search(index=self.indexName, body=queryBody)\n",
    "        return result\n",
    "\n",
    "    def format_search(self, result):\n",
    "        '''\n",
    "        format_search方法对检索结果进行格式化 构建符合要求的字段进行返回\n",
    "        输入result为检索结果 提取其中的检索结果进行后处理\n",
    "        使用ES检索后得到的结果中result['hits']['hits']为数组格式数据\n",
    "        其中每一个元素为一个dict 对应部分字段\n",
    "        '''\n",
    "        docs = result['hits']['hits']\n",
    "        docs = [i['_source'] for i in docs]\n",
    "        targetKeyList = 'P_ID, P_Title, P_Author, P_Publication, P_Organ, P_year, P_Keyword, \\\n",
    "    P_Summary, P_Keyword_seg, P_Title_seg,\\\n",
    "    P_Summary_seg, P_URL, P_Fields, P_Fields_two,P_References, P_Pagecount, P_Page, P_Language,\\\n",
    "    P_Download_num, P_Citation_num,P_Vector,P_Volume, P_Issue,P_Issn,P_Isbn, P_Doi,\\\n",
    "    P_Red1, P_Red2, P_Red3, P_Red4, P_Red5'\n",
    "        targetKeyList = [i.strip() for i in targetKeyList.split(',')]\n",
    "        dict_filter_by_keys = lambda d: {k: d[k] for k in targetKeyList}\n",
    "        dict_filter_text = lambda d: {k if not k == 'text' else 'claim_text': d[k] for k in d}\n",
    "        dict_filter_id = lambda d: {k if not k == '_id' else 'id': d[k] for k in d}\n",
    "        docs = (dict_filter_by_keys(doc) for doc in docs)\n",
    "        docs = [dict_filter_text(doc) for doc in docs]\n",
    "        return docs\n",
    "    def Retrieval(self, titleQuery, kwordQuery, summaryQuery ,pubQuery,fromDate, toDate, volume):\n",
    "        result = self.do_search(titleQuery, kwordQuery, summaryQuery ,pubQuery,fromDate, toDate, volume)\n",
    "        docs = self.format_search(result)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T05:45:51.529546Z",
     "start_time": "2019-07-21T05:45:51.391403Z"
    }
   },
   "outputs": [],
   "source": [
    "class pre_word_count():\n",
    "    def __init__(self,data_path,model_path):\n",
    "        self.data = pd.read_json(data_path)\n",
    "        self.data['authors_seg'] = self.data.apply(lambda r: r['P_Red1'] + r['P_Red3'],axis=1)\n",
    "        self.data['year'] = self.data['P_year'].apply(lambda x:x[:4])\n",
    "        self.model_path = model_path\n",
    "    # 统计出现在句子中的词频\n",
    "    def tf(word,sentence):\n",
    "        c=0\n",
    "        s=sentence.split()\n",
    "        for i in s:\n",
    "            if word==i:\n",
    "                c+=1\n",
    "        return c\n",
    "        \n",
    "    #建立tfidf表\n",
    "    def build_tfidf(max_feature,data):\n",
    "        tfidf=TfidfVectorizer(max_features=max_feature)\n",
    "        f_tfidf=tfidf.fit_transform(data)\n",
    "        f_tfidf=f_tfidf.toarray()\n",
    "        return f_tfidf\n",
    "        \n",
    "    # 统计非重复的所有作者\n",
    "    def tongjiau(self,li):\n",
    "        authors=[]\n",
    "        for i in li:\n",
    "            try:\n",
    "                authors+=i.split()\n",
    "            except:\n",
    "                continue\n",
    "        return list(set(authors))\n",
    "        \n",
    "    # 判断新作者是否在论文中提及热点词\n",
    "    def mention(self,authors,word,sentences,senten_auth):\n",
    "        i=0\n",
    "        for a in authors:\n",
    "            for j in range(len(sentences)):\n",
    "                try:\n",
    "                    if len(list(set(a).intersection(set(senten_auth[j]))))>0 and word in sentences[j]:\n",
    "                        i+=1\n",
    "                except:\n",
    "                    continue\n",
    "        return i\n",
    "        \n",
    "    # 统计某个热点词在某年的跨学科提及数\n",
    "    def multi_journal(word,sentences,journals):\n",
    "        men_journal=[]\n",
    "        for j in range(len(sentences)):\n",
    "            try:\n",
    "                if word in sentences[j]:\n",
    "                    men_journal.append(journals[j])\n",
    "            except:\n",
    "                continue\n",
    "        s=set(men_journal)\n",
    "        if None in s:\n",
    "            s.remove(None)\n",
    "        return len(s)\n",
    "        \n",
    "    # 抽取词频特征\n",
    "    def feature(word,year_df):\n",
    "        abstract_tf=[]\n",
    "        keyword_tf=[]\n",
    "        for y in list(year_df['seg_abstract']):\n",
    "            abstract_tf.append(tf(word,y))\n",
    "        for i in list(year_df['seg_keywords']):\n",
    "            keyword_tf.append(tf(word,i))\n",
    "        return abstract_tf,keyword_tf\n",
    "        \n",
    "    #计算list的平均数\n",
    "\n",
    "    def pingjun(L):\n",
    "\n",
    "        a=reduce(lambda x,y:x+y,L)\n",
    "\n",
    "        return int(a/len(L))\n",
    "    # 获取热点词\n",
    "    def get_hotwords(self,hotword_num):\n",
    "        guanjian=list(map(lambda x:' '.join(x.split()),list(self.data['P_Keyword_seg'])))\n",
    "        merge=' '.join(guanjian)\n",
    "        tr4w = TextRank4Keyword()\n",
    "        tr4w.analyze(text=merge, window=4)\n",
    "        pred_words=[]\n",
    "        for item in tr4w.get_keywords(hotword_num, word_min_len=2):\n",
    "            pred_words.append(item.word)\n",
    "        return pred_words\n",
    "    # 统计每年发表的论文数\n",
    "    def year_paper_num(self,):\n",
    "        year_num=[]\n",
    "        years=list(set(self.data['year']))\n",
    "        group_df=self.data.groupby(by=['year'])\n",
    "        # years.remove(None)\n",
    "        years=sorted([int(i) for i in years])\n",
    "        for y in years:\n",
    "            try:\n",
    "                year_num.append(len(group_df.get_group(str(y))))\n",
    "            except:\n",
    "                continue\n",
    "        return group_df,years,year_num\n",
    "    # 按年份将摘要整合\n",
    "    def groupby_year(self,group_df,years,year_num):\n",
    "        years_df=pd.DataFrame()\n",
    "        years_df['year']=years\n",
    "        seg,key,tit,jou,authors=[],[],[],[],[]\n",
    "\n",
    "        for i in years:\n",
    "            seg.append(' '.join(group_df.get_group(str(i))['P_Summary_seg']))\n",
    "            k=group_df.get_group(str(i))['P_Keyword_seg']\n",
    "            key.append(' '.join(k))\n",
    "            tit.append(' '.join(group_df.get_group(str(i))['P_Title_seg']))\n",
    "            jou.append(' '.join(group_df.get_group(str(i))['P_Publication']))\n",
    "            authors.append(self.tongjiau(group_df.get_group(str(i))['authors_seg']))\n",
    "        years_df['seg_abstract'] = seg\n",
    "        years_df['seg_keywords'] = key\n",
    "        years_df['seg_title'] = tit\n",
    "        years_df['seg_authors'] = authors\n",
    "        years_df['Jounal'] = jou\n",
    "        years_df['paper_num'] = year_num\n",
    "        return years_df\n",
    "    # 统计每年的新作者    \n",
    "    def year_new_author(self,years_df):\n",
    "        authors = list(years_df['seg_authors'])\n",
    "        new_author = [authors[0]]\n",
    "        for i in range(1,len(years_df)):\n",
    "            new=[]\n",
    "            for j in authors[i]:\n",
    "                if j not in authors[i-1]:\n",
    "                    new.append(j)\n",
    "            new_author.append(new)\n",
    "        return new_author\n",
    "    # 统计每年新作者提及数\n",
    "    def year_new_author_mention(self,pred_words,new_author,years):\n",
    "        newauthor_mens=[]\n",
    "        group_df=self.data.groupby(by=['year'])\n",
    "        for word in pred_words:\n",
    "            for i in range(len(new_author)):\n",
    "                newauthor_men=self.mention(new_author,word,list(group_df.get_group(str(years[i]))['P_Summary']),list(map(lambda x:x.split(),list(group_df.get_group(str(years[i]))['seg_authors']))))\n",
    "                newauthor_mens.append(newauthor_men)  \n",
    "        return newauthor_mens\n",
    "    # 统计跨学科提及数\n",
    "    def multi_sub(self,pred_words):\n",
    "        multi_mens = []\n",
    "        group_df = self.data.groupby(by=['year'])\n",
    "        for word in pred_words:\n",
    "            for i in range(len(years)):\n",
    "                journals=multi_journal(word,list(group_df.get_group(str(years[i]))['P_Summary']),list(group_df.get_group(str(years[i]))['Jounal']))\n",
    "                multi_mens.append(journals)\n",
    "        return multi_mens\n",
    "    # 统计每个词初次在关键词中出现的年份\n",
    "    def first_year(self,pred_words,years_df):\n",
    "        f_year=[]\n",
    "        pp=[]\n",
    "        for w in range(len(pred_words)):\n",
    "            for y,i in zip(list(years_df['year']),list(years_df['seg_keywords'])):\n",
    "                if pred_words[w] in i:\n",
    "                    f_year.append(y)\n",
    "                    pp.append(w)\n",
    "                    break\n",
    "        return f_year\n",
    "    #获取每个词18年的特征\n",
    "    def get_feature(self,years_df,pred_words,f_year,newauthor_mens,multi_mens):\n",
    "        year_num=len(years_df)\n",
    "        pp=pd.DataFrame()\n",
    "        for i in range(len(pred_words)):\n",
    "            word=pred_words[i]\n",
    "            fyear=f_year[i]\n",
    "            exist_year=[]\n",
    "            for n,f in zip(list(years_df['year']),[fyear]*year_num):\n",
    "                e=n-f\n",
    "                if e<0:\n",
    "                    exist_year.append(0)\n",
    "                else:\n",
    "                    exist_year.append(e)\n",
    "            dd=pd.DataFrame()\n",
    "            dd['word']=[word]*year_num\n",
    "            dd['f1']=feature(word,years_df)[0]\n",
    "            dd['f2']=feature(word,years_df)[1]\n",
    "            dd['f3'] = exist_year\n",
    "            dd['f4'] = newauthor_mens[i*year_num:i*year_num+(year_num-1)]+[pingjun(newauthor_mens[i*year_num+(year_num-1)-3:i*year_num+(year_num-1)])]\n",
    "            dd['f6'] = multi_mens[i*year_num:i*year_num+(year_num-1)]+[pingjun(multi_mens[i*year_num+(year_num-1)-3:i*year_num+(year_num-1)])]\n",
    "            if i==0:\n",
    "                pp=dd\n",
    "            else:\n",
    "                pp=pp.append(dd)\n",
    "            pp=pp.reset_index(drop=True)\n",
    "            return pp\n",
    "    #预测某个词的词频\n",
    "    def zhidingword(search_word,test_data,model):\n",
    "        pre_data=test_data.loc[test_data.word==search_word].iloc[:,1:-1]\n",
    "        result = model.predict(pre_data)\n",
    "        return int(result)\n",
    "    #返回结果\n",
    "    def pre_result(self):\n",
    "        model_GBR=joblib.load(self.model_path)\n",
    "        pred_words=self.get_hotwords(100)\n",
    "        years=self.year_paper_num()[1]\n",
    "        group_df=self.year_paper_num()[0]\n",
    "        year_num=self.year_paper_num()[2]\n",
    "        years_df=self.groupby_year(group_df,years,year_num)\n",
    "        f_year=self.first_year(pred_words,years_df)\n",
    "        new_author=self.year_new_author(years_df)\n",
    "        newauthor_mens=self.year_new_author_mention(pred_words,new_author,years)\n",
    "        multi_mens=self.multi_sub(pred_words)\n",
    "        pp=self.get_feature(years_df,pred_words,f_year,newauthor_mens,multi_mens)\n",
    "        for word in pred_words:\n",
    "            result=self.zhidingword(word,test_data,model_GBR)\n",
    "            search_ke=list(pp.loc[pp.word==word]['f1'])+[result]\n",
    "        result=pd.DataFrame()\n",
    "        result['word']=pred_words\n",
    "        result['year']=years\n",
    "        result['count']=search_ke\n",
    "        result=result.to_json(orient='index')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T05:45:51.782388Z",
     "start_time": "2019-07-21T05:45:51.777113Z"
    }
   },
   "outputs": [],
   "source": [
    "esPaperObj = esPaperRetrieval(host='10.8.128.205',port=49200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T05:45:52.368493Z",
     "start_time": "2019-07-21T05:45:52.231807Z"
    }
   },
   "outputs": [],
   "source": [
    "data = esPaperObj.Retrieval(titleQuery='',kwordQuery='',\n",
    "                     summaryQuery='中医',pubQuery='',fromDate='2001-01-01',toDate='2018-01-01',volume=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T05:45:58.391123Z",
     "start_time": "2019-07-21T05:45:58.312837Z"
    }
   },
   "outputs": [],
   "source": [
    "aaa = pre_word_count(json.dumps(data), '../data/paper_data/paper_hotpoint.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T05:46:01.329387Z",
     "start_time": "2019-07-21T05:45:59.363032Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.254 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'seg_authors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/env/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'seg_authors'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a93c97a8e697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maaa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-74caedfdf5a7>\u001b[0m in \u001b[0;36mpre_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mf_year\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myears_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mnew_author\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear_new_author\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mnewauthor_mens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear_new_author_mention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_author\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0mmulti_mens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mpp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_year\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnewauthor_mens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmulti_mens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-74caedfdf5a7>\u001b[0m in \u001b[0;36myear_new_author_mention\u001b[0;34m(self, pred_words, new_author, years)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_author\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mnewauthor_men\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_author\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'P_Summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seg_authors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0mnewauthor_mens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewauthor_men\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnewauthor_mens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'seg_authors'"
     ]
    }
   ],
   "source": [
    "aaa.pre_result() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
