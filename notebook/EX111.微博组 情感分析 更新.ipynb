{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、情感计算更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T05:44:21.509220Z",
     "start_time": "2019-07-25T05:44:21.232135Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from jieba import analyse\n",
    "import pickle\n",
    "import os\n",
    "import jieba\n",
    "from jieba import analyse\n",
    "from snownlp import SnowNLP\n",
    "from snownlp.sentiment import Sentiment\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T05:44:21.525101Z",
     "start_time": "2019-07-25T05:44:21.511511Z"
    }
   },
   "outputs": [],
   "source": [
    "class esWeiboTweetRetrieval():\n",
    "    '''\n",
    "    根据论文检索需求进行功能的微调\n",
    "    '''\n",
    "    _instance = None\n",
    "    _first_init = True\n",
    "    def __new__(cls, *args, **kw):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(esWeiboTweetRetrieval, cls).__new__(cls)  \n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, host, port):\n",
    "        '''\n",
    "        使用ES进行论文检索 指定host、port以及专利index之后进行检索\n",
    "        '''\n",
    "        super(esWeiboTweetRetrieval, self).__init__()\n",
    "        self.es = Elasticsearch(hosts=host, port=port, timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "        self.indexName = 'weibo-tweet-index'\n",
    "\n",
    "    def do_search(self, W_search_query, W_content, volume):\n",
    "        '''\n",
    "        do_search方法执行具体检索过程\n",
    "        wordQuery 本应为查询对应微博时所用的检索词 此处暂时不用 目前暂时只检索微博评论数据后续再修改对象为\n",
    "        \n",
    "        volume为每次检索返回的数目\n",
    "        '''\n",
    "        queryBody = {\n",
    "        \"query\": {\n",
    "        \"bool\": {\n",
    "        \"must\": [ ],\n",
    "        \"must_not\": [ ],\n",
    "        \"should\": [\n",
    "        {\n",
    "        \"query_string\": {\n",
    "        \"default_field\": \"W_content\",\n",
    "        \"query\": W_content\n",
    "        }\n",
    "        }\n",
    "        ,\n",
    "        {\n",
    "        \"query_string\": {\n",
    "        \"default_field\": \"W_search_query\",\n",
    "        \"query\": W_search_query\n",
    "        }\n",
    "        }\n",
    "        ]\n",
    "        }\n",
    "        },\n",
    "        \"from\": 0,\n",
    "        \"size\": volume,\n",
    "        \"sort\": [ ],\n",
    "        \"aggs\": { }\n",
    "        }\n",
    "        result = self.es.search(index=self.indexName, body=queryBody)\n",
    "        return result\n",
    "\n",
    "    def format_search(self, result):\n",
    "        '''\n",
    "        format_search方法对检索结果进行格式化 构建符合要求的字段进行返回\n",
    "        输入result为检索结果 提取其中的检索结果进行后处理\n",
    "        使用ES检索后得到的结果中result['hits']['hits']为数组格式数据\n",
    "        其中每一个元素为一个dict 对应部分字段\n",
    "        '''\n",
    "        docs = result['hits']['hits']\n",
    "        docs = [i['_source'] for i in docs]\n",
    "        targetKeyList = 'W_ID,W_weibo_id,W_search_query,W_nick_name,W_weibo_url,W_created_at,\\\n",
    "        W_like_num,W_repost_num,W_comment_num,W_content,W_user_id,W_image_url,W_video_url,W_tool,W_location,\\\n",
    "        W_location_map_info,W_origin_weibo_url,W_origin_weibo_content,W_crawl_time'\n",
    "        targetKeyList = [i.strip() for i in targetKeyList.split(',')]\n",
    "        dict_filter_by_keys = lambda d: {k: d[k] for k in targetKeyList}\n",
    "        dict_filter_id = lambda d: {k if not k == '_id' else 'id': d[k] for k in d}\n",
    "        docs = list(dict_filter_by_keys(doc) for doc in docs)\n",
    "        return docs\n",
    "    def Retrieval(self, W_search_query, W_content, volume):\n",
    "        result = self.do_search(W_search_query, W_content, volume)\n",
    "        docs = self.format_search(result)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T05:44:37.924245Z",
     "start_time": "2019-07-25T05:44:37.873947Z"
    }
   },
   "outputs": [],
   "source": [
    "# input： 微博正文检索结果\n",
    "# output： {\n",
    "#         dict_emo         微博情感七分类得分\n",
    "#         content_emo      微博情感七分类对应各5条微博\n",
    "#         dict_senti          微博情感二分类得分\n",
    "#         content_senti   负向微博前5条  及 正向微博前5条\n",
    "# }\n",
    "def get_weibo_senti(result):\n",
    "    conment_list = lambda x:x[:5] if len(x)>5 else x\n",
    "    happy = [\"PA\",\"PE\"]#乐\n",
    "    like = [\"PD\",\"PH\",\"PG\",\"PB\",\"PK\"]#好\n",
    "    anger = [\"NA\"]#怒\n",
    "    sad = [\"NB\",\"NJ\",\"NH\",\"PF\"]#哀\n",
    "    afraid = [\"NI\",\"NC\",\"NG\"]#惧\n",
    "    boring = [\"NE\",\"ND\",\"NN\",\"NK\",\"NL\"]#恶\n",
    "    suprise = [\"PC\"]#惊\n",
    "    emotion = pd.read_excel('../data/weibo_data/senti onthology.xlsx', header=0, encoding='utf8')\n",
    "    words = emotion[\"词语\"].tolist()\n",
    "    emo = emotion[\"情感分类\"].tolist()\n",
    "    core = emotion[\"强度\"].tolist()\n",
    "    pos_neg = emotion[\"极性\"].tolist()\n",
    "    res = [i['W_content'] for i in result]\n",
    "    num_happy=0\n",
    "    num_like =0\n",
    "    num_anger=0\n",
    "    num_sad = 0\n",
    "    num_afraid = 0\n",
    "    num_boring = 0\n",
    "    num_suprise = 0\n",
    "    num_pos=0\n",
    "    num_neg=0\n",
    "    happy_content = []\n",
    "    like_content = []\n",
    "    anger_content = []\n",
    "    sad_content = []\n",
    "    afraid_content = []\n",
    "    boring_content = []\n",
    "    suprise_content = []\n",
    "    pos_content = []\n",
    "    neg_content = []\n",
    "    if len(res)>500:\n",
    "        N=500\n",
    "    else:\n",
    "        N = len(res)\n",
    "    for i in res[:N]: \n",
    "        sore_happy=0\n",
    "        sore_like =0\n",
    "        sore_anger=0\n",
    "        sore_sad = 0\n",
    "        sore_afraid = 0\n",
    "        sore_boring = 0\n",
    "        sore_suprise = 0\n",
    "        sore_pos=0\n",
    "        sore_neg=0\n",
    "        if len(i)>4:\n",
    "            for word in jieba.lcut(i):\n",
    "                if word in emotion[\"词语\"].tolist():\n",
    "                    aa = emotion.loc[emotion['词语']==word]\n",
    "                    if aa[\"情感分类\"].all() in happy:                  \n",
    "                         sore_happy= aa[\"强度\"].tolist()[0]+sore_happy\n",
    "                    elif aa[\"情感分类\"].all() in like:\n",
    "                        sore_like = aa[\"强度\"].tolist()[0]+sore_like\n",
    "                    elif aa[\"情感分类\"].all() in anger:\n",
    "                        sore_anger = aa[\"强度\"].tolist()[0]+sore_anger\n",
    "                    elif aa[\"情感分类\"].all() in sad:\n",
    "                        sore_sad = aa[\"强度\"].tolist()[0]+sore_sad\n",
    "                    elif aa[\"情感分类\"].all() in afraid:\n",
    "                        sore_afraid = aa[\"强度\"].tolist()[0]+sore_afraid\n",
    "                    elif aa[\"情感分类\"].all() in boring:\n",
    "                        sore_boring = aa[\"强度\"].tolist()[0]+sore_boring\n",
    "                    elif aa[\"情感分类\"].all() in suprise:\n",
    "                        sore_suprise = aa[\"强度\"].tolist()[0]+sore_suprise\n",
    "                    if aa[\"极性\"].tolist()[0] == 1:\n",
    "                        sore_pos = aa[\"强度\"].tolist()[0]+sore_pos\n",
    "                    elif aa[\"极性\"].tolist()[0] == 2:\n",
    "                        sore_neg = aa[\"强度\"].tolist()[0]+sore_neg\n",
    "            emo=[]\n",
    "            emo.append(sore_happy)\n",
    "            emo.append(sore_like)\n",
    "            emo.append(sore_anger)\n",
    "            emo.append(sore_sad)\n",
    "            emo.append(sore_afraid)\n",
    "            emo.append(sore_boring)\n",
    "            emo.append(sore_suprise)\n",
    "            senti = []\n",
    "            senti.append(sore_pos)\n",
    "            senti.append(sore_neg)\n",
    "            if max(emo) == sore_happy:\n",
    "                \n",
    "                num_happy = num_happy+1\n",
    "                if sore_happy>10:\n",
    "                    happy_content.append(i)\n",
    "            elif max(emo) == sore_like:\n",
    "                if sore_like>15:\n",
    "                    like_content.append(i)\n",
    "                num_like = num_like+1\n",
    "            elif max(emo) == sore_anger:\n",
    "               \n",
    "                anger_content.append(i)\n",
    "                num_anger = num_anger+1\n",
    "            elif max(emo) == sore_sad:\n",
    "                \n",
    "                sad_content.append(i)\n",
    "                num_sad = num_sad+1\n",
    "            elif max(emo) == sore_afraid:\n",
    "                afraid_content.append(i)\n",
    "                num_afraid = num_afraid+1\n",
    "            elif max(emo) == sore_boring:\n",
    "\n",
    "                boring_content.append(i)\n",
    "                num_boring = num_boring+1\n",
    "            elif max(emo) == sore_suprise:\n",
    "\n",
    "                suprise_content.append(i)\n",
    "                num_suprise = num_suprise+1\n",
    "            if max(senti) == sore_pos:\n",
    "                if sore_pos>14:\n",
    "                    pos_content.append(i)\n",
    "                num_pos = num_pos+1\n",
    "            else:\n",
    "                if sore_neg>14:\n",
    "                    neg_content.append(i)\n",
    "                num_neg = num_neg+1\n",
    "            sum1 = num_happy+num_like+num_anger+num_sad+num_afraid+num_boring+num_suprise\n",
    "            dict_emo = {\"happy\":round(num_happy/sum1,3),\"like\":round(num_like/sum1,3),\"anger\":round(num_anger/sum1,3),\n",
    "                        \"sad\":round(num_sad/sum1,3),\"afraid\":round(num_afraid/sum1,3),\"boring\":round(num_boring/sum1,3),\n",
    "                        \"suprise\":round(num_suprise/sum1,3)}\n",
    "            dict_senti = {\"positive\":round(num_pos/(num_pos+num_neg),3),\"negtive\":round(num_neg/(num_pos+num_neg),3)}\n",
    "            content_emo = {\"happy\":conment_list(happy_content),\"like\":conment_list(like_content),\"anger\":conment_list(anger_content),\n",
    "                        \"sad\":conment_list(sad_content),\"afraid\":conment_list(afraid_content),\"boring\":conment_list(boring_content),\n",
    "                        \"suprise\":conment_list(suprise_content)}\n",
    "            content_senti = {\"positive\":conment_list(pos_content),\"negtive\":conment_list(neg_content)}\n",
    "    return dict_emo,content_emo,dict_senti,content_senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T06:25:06.094307Z",
     "start_time": "2019-07-25T06:24:59.876627Z"
    }
   },
   "outputs": [],
   "source": [
    "esObj = esWeiboTweetRetrieval(host='10.8.128.205',port=49200)\n",
    "docs = esObj.Retrieval(W_search_query='垃圾分类',W_content='垃圾分类',volume=100)\n",
    "dict_emo,content_emo,dict_senti,content_senti = get_weibo_senti(docs)\n",
    "\n",
    "make_dict = lambda x:[{'name':k,'value':x[k]} for k in x]\n",
    "\n",
    "dict_senti = make_dict(dict_senti)\n",
    "dict_emo = make_dict(dict_emo)\n",
    "\n",
    "result = {'dict_senti':dict_senti,\n",
    "         'dict_emo':dict_emo,\n",
    "         'content_senti':content_senti,\n",
    "         'content_emo':content_emo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T06:25:06.377240Z",
     "start_time": "2019-07-25T06:25:06.268460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dict_senti': [{'name': 'positive', 'value': 0.85},\n",
       "  {'name': 'negtive', 'value': 0.15}],\n",
       " 'dict_emo': [{'name': 'happy', 'value': 0.52},\n",
       "  {'name': 'like', 'value': 0.3},\n",
       "  {'name': 'anger', 'value': 0.0},\n",
       "  {'name': 'sad', 'value': 0.0},\n",
       "  {'name': 'afraid', 'value': 0.16},\n",
       "  {'name': 'boring', 'value': 0.02},\n",
       "  {'name': 'suprise', 'value': 0.0}],\n",
       " 'content_senti': {'positive': ['#垃圾分类# 【农村垃圾分类进行时】永宁街道召开垃圾分类工作再推进会议，要求各社区和相关部门整体推进垃圾分类工作、完善垃圾分类各类设施、加大垃圾分类宣传力度，重点落实后期居民垃圾分类工作的运行。',\n",
       "   '#垃圾分类# 【图解：谁是真正的“垃圾之王”？】受到垃圾分类在上海的落地实施，以及生活垃圾分类入法等消息刺激，A股催生出了全新的概念板块：垃圾分类板块。6月25日早盘，垃圾分类概念迅速拉升。然而，谁才是垃圾分类中真正的王牌企业？中国\"科\"公司评选之垃圾分类板块:谁才是\"垃圾之王\"?',\n",
       "   '#垃圾分类# 【图解：谁是真正的“垃圾之王”？】受到垃圾分类在上海的落地实施，以及生活垃圾分类入法等消息刺激，A股催生出了全新的概念板块：垃圾分类板块。6月25日早盘，垃圾分类概念迅速拉升。然而，谁才是垃圾分类中真正的王牌企业？中国\"科\"公司评选之垃圾分类板块:谁才是\"垃圾之王\"?',\n",
       "   '#垃圾分类# 为进一步关普及垃圾分类的理念和知识，近期，市城治办开展了垃圾分类进机关的宣讲活动。此次活动通过现场垃圾分类知识的讲解，介绍垃圾分类的方法和必要性，就生活垃圾分类、厨余垃圾就地减量等方面介绍了一些成功经验，将垃圾分类工作结合到日常的工作中去，真正让垃圾分类理念入心入脑，为我市垃圾分类工作和为城市生活垃圾减量化、资源化、无害化打下坚实基础。[组图共2张]',\n",
       "   '#垃圾分类# 今天，张江镇继续拓展垃圾分类宣传阵地。镇总工会组织各下属基层单位工会主席、工会干部共63人，开展垃圾分类培训讲座，普及办公场所垃圾分类知识；在孙桥路居委、沔北村委开展垃圾分类现场演练；在藿香路居委开展垃圾分类活动，全面提升居民的垃圾分类认识；钱堂村召开垃圾分类部署会。截止目前，我镇共组织垃圾分类培训及活动86场，人数达12300余人。[组图共5张]'],\n",
       "  'negtive': ['#垃圾分类# 很多人对垃圾分类有这样一种误解：以为在家里分好类的垃圾，会被垃圾车一车全部运走。其实不然，咱们不仅从源头上分类，在终端方面，也会分开装载，去往不同的处理点。另外，一个小提示哦，咱们南京的垃圾分类，不分干湿垃圾，而是采用“三分法”，即可回收垃圾，有毒有害垃圾和其他垃圾。 http://t.cn/Ai0uL2ZB',\n",
       "   '#垃圾分类#【我们不一样！成都不分干垃圾，湿垃圾！】 今天，成都发布《垃圾分类草案》修改意见建议。 上海垃圾分类包括干垃圾、湿垃圾、可回收物、有害垃圾。 最为抓狂的，主要是干垃圾和湿垃圾的区分。 相比上海，成都明显要人性化得多，分为四类：可回收物、有害垃圾、餐厨垃圾、其他垃圾。 《草案》意见征集时间是截止2019年8月10日，也就是垃圾分类正式实施最快也要8月10日以后。 #成都爆料#',\n",
       "   '【#干垃圾还是湿垃圾#？一张图教你成为垃圾分类专家！[喵喵]】今天，《上海市生活垃圾管理条例》将正式施行。可回收物、有害垃圾、湿垃圾、干垃圾，据说上海人民已经要被#垃圾分类#“逼疯”……垃圾应该怎么扔？戳图↓看完再来套题巩固知识，包你成为垃圾（分类）专家！转！ #垃圾分类成上海新社交#（央视新闻）',\n",
       "   '【你是个什么垃圾？】#垃圾分类# 北京也要开始啦！罚款不会低于上海。北京的垃圾分类标准与上海并不完全一致，分为厨余垃圾、有害垃圾、其它垃圾及可回收物。那么垃圾分类，广州、深圳还会远吗？[允悲] 那区分垃圾的口诀，你都知道哪些呢？ 来[来]一招记住垃圾分类： 猪能吃的是湿垃圾 猪不能吃的的是干垃圾 猪吃了会shi的是有害垃圾 可以卖出去换猪的是可回收垃圾 [猪头] 保护环境，人人有责，垃圾分类，让中国变得更美！[围观][组图共3张]']},\n",
       " 'content_emo': {'happy': ['#垃圾分类# 今天，张江镇继续拓展垃圾分类宣传阵地。镇总工会组织各下属基层单位工会主席、工会干部共63人，开展垃圾分类培训讲座，普及办公场所垃圾分类知识；在孙桥路居委、沔北村委开展垃圾分类现场演练；在藿香路居委开展垃圾分类活动，全面提升居民的垃圾分类认识；钱堂村召开垃圾分类部署会。截止目前，我镇共组织垃圾分类培训及活动86场，人数达12300余人。[组图共5张]',\n",
       "   '#垃圾分类#【开展分类宣传  共促绿色生活】近日，余杭区城乡环境卫生监管中心的工作人员分别来到星桥中心小学、南苑街道文仪社区社区开展垃圾分类宣传活动。向学校老师、向社区居民讲解垃圾分类的意义、如何开展垃圾分类，让大家更进一步的了解垃圾分类、推广垃圾分类。 [组图共4张]'],\n",
       "  'like': ['#垃圾分类# 【农村垃圾分类进行时】永宁街道召开垃圾分类工作再推进会议，要求各社区和相关部门整体推进垃圾分类工作、完善垃圾分类各类设施、加大垃圾分类宣传力度，重点落实后期居民垃圾分类工作的运行。',\n",
       "   '#垃圾分类# 【图解：谁是真正的“垃圾之王”？】受到垃圾分类在上海的落地实施，以及生活垃圾分类入法等消息刺激，A股催生出了全新的概念板块：垃圾分类板块。6月25日早盘，垃圾分类概念迅速拉升。然而，谁才是垃圾分类中真正的王牌企业？中国\"科\"公司评选之垃圾分类板块:谁才是\"垃圾之王\"?',\n",
       "   '#垃圾分类# 【图解：谁是真正的“垃圾之王”？】受到垃圾分类在上海的落地实施，以及生活垃圾分类入法等消息刺激，A股催生出了全新的概念板块：垃圾分类板块。6月25日早盘，垃圾分类概念迅速拉升。然而，谁才是垃圾分类中真正的王牌企业？中国\"科\"公司评选之垃圾分类板块:谁才是\"垃圾之王\"?',\n",
       "   '#垃圾分类# 根据垃圾分类工作要求，永宁城管持续做好垃圾分类宣传工作，向沿街商铺、机关事业单位发放全市队伍垃圾分类执法保障一封信《推行垃圾分类  共创美好南京》，并要求签订《关于按照标准分类投放垃圾的承诺书》。 [组图共4张]',\n",
       "   '【和上海标准不一样！台州市分类办权威指南在这里】这段时间，“上海”和“垃圾分类”无疑成了时下热点关键词。大家大呼“快被垃圾分类逼疯”！！记者从市分类办获悉，早在去年4月，台州就开始推行垃圾分类，“垃圾分类”理念已经深入人心，但也存在“不知如何分类”的尴尬。记者从市分类办了解到，台州目前延用的垃圾分类标准是新国标规范，将垃圾分为可回收垃圾、有害垃圾、易腐垃圾和其他垃圾四大类，采用蓝绿红灰4种颜色进行区分，与上海的分类标准并不一样。http://t.cn/AiObIHiw #台州身边事##台州爆料##垃圾分类##上海一超市垃圾混投被罚3万##它们不会垃圾分类但我们可以#[组图共6张]'],\n",
       "  'anger': [],\n",
       "  'sad': [],\n",
       "  'afraid': ['#垃圾分类#【#干垃圾还是湿垃圾#？一张图教你成为垃圾分类专家！[喵喵]】今天《上海市生活垃圾管理条例》正式施行。可回收物、有害垃圾、湿垃圾、干垃圾，据说上海人民已经要被垃圾分类“逼疯”……垃圾应该怎么扔？戳长图↓包你成为垃圾（分类）专家！',\n",
       "   '这是监控垃圾分类？厉害了[摊手]  #上海垃圾分类个人扔错罚款##垃圾分类##垃圾分类就是新时尚# 曾经的道哥的秒拍视频',\n",
       "   '#垃圾分类#【#干垃圾还是湿垃圾#？一张图教你成为垃圾分类专家！[喵喵]】今天《上海市生活垃圾管理条例》正式施行，北京也将实施。可回收物、有害垃圾、湿垃圾、干垃圾，据说上海人民已经要被垃圾分类“逼疯”……垃圾应该怎么扔？戳长图↓包你成为垃圾（分类）专家！',\n",
       "   '#垃圾分类# 很多人对垃圾分类有这样一种误解：以为在家里分好类的垃圾，会被垃圾车一车全部运走。其实不然，咱们不仅从源头上分类，在终端方面，也会分开装载，去往不同的处理点。另外，一个小提示哦，咱们南京的垃圾分类，不分干湿垃圾，而是采用“三分法”，即可回收垃圾，有毒有害垃圾和其他垃圾。 http://t.cn/Ai0uL2ZB',\n",
       "   '#垃圾分类# 收下这份垃圾分类投放指南，长春的你迟早用得到！不仅囊括了干垃圾、湿垃圾、可回收物、有害垃圾四分类，还贴心地列举了大件垃圾、装修垃圾、电子废弃物等类别下的物品。非常实用！[思考] #垃圾分类就是新时尚# 长春 显示地图[组图共9张]'],\n",
       "  'boring': ['#垃圾分类#它来了，它来了，它带着各种名称的垃圾桶走来了?（南信大也有分类垃圾的垃圾桶咯）今天，你有没有被垃圾分类搞到崩溃[允悲] [组图共2张]',\n",
       "   '最近不是都在垃圾分类吗？咱们也给渣男分分类~   #垃圾分类# 体院小哥的微博视频'],\n",
       "  'suprise': []}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
