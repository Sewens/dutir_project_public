{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "import jieba\n",
    "import pymysql\n",
    "from jieba import analyse\n",
    "import jieba.analyse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class esWeiboCommentRetrieval():\n",
    "    '''\n",
    "    根据论文检索需求进行功能的微调\n",
    "    '''\n",
    "    _instance = None\n",
    "    _first_init = True\n",
    "    def __new__(cls, *args, **kw):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(esWeiboCommentRetrieval, cls).__new__(cls)  \n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, host, port):\n",
    "        '''\n",
    "        使用ES进行论文检索 指定host、port以及专利index之后进行检索\n",
    "        '''\n",
    "        super(esWeiboCommentRetrieval, self).__init__()\n",
    "        self.es = Elasticsearch(hosts=host, port=port, timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "        self.indexName = 'weibo-comment-index'\n",
    "\n",
    "    def do_search(self, wordQuery, textQuery, volume):\n",
    "        '''\n",
    "        do_search方法执行具体检索过程\n",
    "        wordQuery 本应为查询对应微博时所用的检索词 此处暂时不用 目前暂时只检索微博评论数据后续再修改对象为\n",
    "        \n",
    "        volume为每次检索返回的数目\n",
    "        '''\n",
    "        queryBody = {\n",
    "          \"query\": {\n",
    "            \"match\": {\n",
    "              \"C_content\": textQuery\n",
    "            }\n",
    "          },\n",
    "          \"from\": 0,\n",
    "          \"size\": volume,\n",
    "          \"sort\": [],\n",
    "          \"aggs\": {}\n",
    "        }\n",
    "        result = self.es.search(index=self.indexName, body=queryBody)\n",
    "        return result\n",
    "\n",
    "    def format_search(self, result):\n",
    "        '''\n",
    "        format_search方法对检索结果进行格式化 构建符合要求的字段进行返回\n",
    "        输入result为检索结果 提取其中的检索结果进行后处理\n",
    "        使用ES检索后得到的结果中result['hits']['hits']为数组格式数据\n",
    "        其中每一个元素为一个dict 对应部分字段\n",
    "        '''\n",
    "        docs = result['hits']['hits']\n",
    "        docs = [i['_source'] for i in docs]\n",
    "        targetKeyList = 'C_ID,C_comment_id,C_comment_user_id,C_comment_user_nick_name,C_content,\\\n",
    "        C_weibo_url,C_like_num,C_created_at,C_crawl_time'\n",
    "        targetKeyList = [i.strip() for i in targetKeyList.split(',')]\n",
    "        dict_filter_by_keys = lambda d: {k: d[k] for k in targetKeyList}\n",
    "        dict_filter_text = lambda d: {k if not k == 'text' else 'claim_text': d[k] for k in d}\n",
    "        dict_filter_id = lambda d: {k if not k == '_id' else 'id': d[k] for k in d}\n",
    "        docs = (dict_filter_by_keys(doc) for doc in docs)\n",
    "        docs = [dict_filter_text(doc) for doc in docs]\n",
    "        return docs\n",
    "    def Retrieval(self, wordQuery, textQuery, volume):\n",
    "        result = self.do_search(wordQuery, textQuery, volume)\n",
    "        docs = self.format_search(result)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {'wordQuery':'',\n",
    "        'textQuery':'垃圾分类',\n",
    "        'volume':1000}\n",
    "baseUrl = 'http://10.8.128.205:29280/Lawbda/dataWare/1.0.0/weibo/search'\n",
    "docs = requests.get(baseUrl,params=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeiboEventAnalysis:\n",
    "    def __init__(self,):\n",
    "        self.es = esWeiboCommentRetrieval(host='10.8.128.205',port=49200)\n",
    "    def do_search(self,kword):\n",
    "        result = self.es.Retrieval(wordQuery=kword, textQuery=kword, volume=1000)\n",
    "        return result\n",
    "\n",
    "    def get_event_words(self,docs):\n",
    "        base_dict_path = './'\n",
    "        dict_file_list = [\"SogouLabDic.txt\",\"dict_baidu_utf8.txt\",\"dict_pangu.txt\",\n",
    "                          \"dict_sougou_utf8.txt\",\"dict_tencent_utf8.txt\",\"my_dict.txt\"]\n",
    "        dict_file_path = [os.path.join(base_dict_path,fname) for fname in dict_file_list]\n",
    "        for p in dict_file_path:jieba.load_userdict(p)\n",
    "        stopwords = {}.fromkeys([ line.rstrip() for line in open('Stopword.txt','r',encoding='utf-8')])\n",
    "        ID = []\n",
    "        text = []\n",
    "        ID = [i['C_ID'] for i in docs]\n",
    "        text =[i['C_content'] for i in docs]\n",
    "        # cut words\n",
    "        result=[]\n",
    "        for i in text:\n",
    "            if i != False:\n",
    "                seg = jieba.cut(i)\n",
    "                for j in seg:\n",
    "                    if j not in stopwords:  \n",
    "                        result.append(j)\n",
    "                result.append('\\n')\n",
    "            else:\n",
    "                continue\n",
    "        #extract keywords\n",
    "        keywords_result=[]\n",
    "        tfidf = analyse.extract_tags\n",
    "        for line in result:\n",
    "            text = line\n",
    "            keywords = tfidf(text,allowPOS=('ns','nr','nt','nz','nl','n', 'vn','vd','vg','v','vf','a','an','i'))\n",
    "            for keyword in keywords:\n",
    "                keywords_result.append(keyword)\n",
    "        return keywords_result\n",
    "    \n",
    "    def keywords_TextRank(self,keywords):\n",
    "\n",
    "        lyric= ''\n",
    "        for i in keywords:\n",
    "            lyric+=i\n",
    "        result=jieba.analyse.textrank(lyric,topK=50,withWeight=True)\n",
    "        result2 = result[:10]\n",
    "        w_keywords_50 = {i[0]:i[1] for i in result}\n",
    "        w_keywords_10 = {i[0]:i[1] for i in result2}\n",
    "        return w_keywords_50, w_keywords_10\n",
    "    \n",
    "    def get_sentiment_percentage(self,keywords):\n",
    "        comment = []\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        fracs = {}\n",
    "        for line_data in keywords:\n",
    "            comment = line_data\n",
    "            s = SnowNLP(comment)\n",
    "            rates = s.sentiments    \n",
    "            if (rates >= 0.5):\n",
    "                pos_count += 1\n",
    "            elif (rates < 0.5):\n",
    "                neg_count += 1\n",
    "            else :\n",
    "                pass\n",
    "        pos_rate=pos_count/(pos_count + neg_count)\n",
    "        neg_rate=neg_count/(pos_count + neg_count)\n",
    "        fracs = {'积极': pos_rate ,'消极': neg_rate}\n",
    "        return fracs\n",
    "    def post_process(self,word_dict):\n",
    "        return [{'name':k,'value':word_dict[k]} for k in word_dict]\n",
    "        \n",
    "    def SearchNAnalysis(self,kword):\n",
    "        docs = self.do_search(kword)\n",
    "        keywords_result = self.get_event_words(docs)\n",
    "        w_keywords_50, w_keywords_10 = self.keywords_TextRank(keywords_result)\n",
    "        fracs = self.get_sentiment_percentage(keywords_result)\n",
    "        response = {\n",
    "            'kwordTop50Cloud':w_keywords_50,\n",
    "            'kwordTop10Bar':w_keywords_10,\n",
    "            'sentimentPie':fracs\n",
    "        }\n",
    "        response = {k:self.post_process(response[k]) for k in response}\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp = WeiboEventAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kwordTop50Cloud': [{'name': '垃圾', 'value': 1.0},\n",
       "  {'name': '分类', 'value': 0.7886273276514632},\n",
       "  {'name': '洋垃圾', 'value': 0.18072583730500644},\n",
       "  {'name': '超话', 'value': 0.04908631579908009},\n",
       "  {'name': '垃圾桶', 'value': 0.04870221337287133},\n",
       "  {'name': '哥哥', 'value': 0.03379394192200975},\n",
       "  {'name': '回收', 'value': 0.025470424038297725},\n",
       "  {'name': '跟着', 'value': 0.024755504905345397},\n",
       "  {'name': '环境', 'value': 0.023755389264296333},\n",
       "  {'name': '小心', 'value': 0.02371932582171862},\n",
       "  {'name': '评论', 'value': 0.022764172476962156},\n",
       "  {'name': '同款', 'value': 0.02242245495460441},\n",
       "  {'name': '微笑', 'value': 0.021680423024325872},\n",
       "  {'name': '老师', 'value': 0.021526182222226076},\n",
       "  {'name': '中国', 'value': 0.020987584078350194},\n",
       "  {'name': '鼓掌', 'value': 0.018942967718809357},\n",
       "  {'name': '南京', 'value': 0.018919587660372456},\n",
       "  {'name': '投放', 'value': 0.018676956886822773},\n",
       "  {'name': '口红', 'value': 0.018146923935315768},\n",
       "  {'name': '配图', 'value': 0.017700255821726477},\n",
       "  {'name': '上海', 'value': 0.017143679317312758},\n",
       "  {'name': '挑战', 'value': 0.017023724522347544},\n",
       "  {'name': '城市', 'value': 0.01602555362420315},\n",
       "  {'name': '领头羊', 'value': 0.015505217263626865},\n",
       "  {'name': '爱护', 'value': 0.0149491951347494},\n",
       "  {'name': '保护环境', 'value': 0.01462820502666194},\n",
       "  {'name': '不用', 'value': 0.014543609143951574},\n",
       "  {'name': '发布', 'value': 0.014187806284986898},\n",
       "  {'name': '垃圾袋', 'value': 0.013915346310428088},\n",
       "  {'name': '国家', 'value': 0.013599472033778658},\n",
       "  {'name': '教学', 'value': 0.013597509331814694},\n",
       "  {'name': '宣传', 'value': 0.01350912078999697},\n",
       "  {'name': '可爱', 'value': 0.013319444654749253},\n",
       "  {'name': '深圳', 'value': 0.013285212568966793},\n",
       "  {'name': '实行', 'value': 0.01321177637501708},\n",
       "  {'name': '北京', 'value': 0.012971903504586099},\n",
       "  {'name': '担心', 'value': 0.012748561005590932},\n",
       "  {'name': '南京城', 'value': 0.012601205825691618},\n",
       "  {'name': '憧憬', 'value': 0.012411678385624294},\n",
       "  {'name': '喜欢', 'value': 0.012146858698279424},\n",
       "  {'name': '电池', 'value': 0.011926072634915633},\n",
       "  {'name': '工作', 'value': 0.011905918328008417},\n",
       "  {'name': '城管', 'value': 0.011822477636190697},\n",
       "  {'name': '香港', 'value': 0.011612141496565769},\n",
       "  {'name': '干电池', 'value': 0.011527780893649068},\n",
       "  {'name': '社区', 'value': 0.011441540471281389},\n",
       "  {'name': '做起', 'value': 0.011236591027366136},\n",
       "  {'name': '新时尚', 'value': 0.010866801542717307},\n",
       "  {'name': '扩容', 'value': 0.010659154103314972},\n",
       "  {'name': '关注', 'value': 0.01061606745392328}],\n",
       " 'kwordTop10Bar': [{'name': '垃圾', 'value': 1.0},\n",
       "  {'name': '分类', 'value': 0.7886273276514632},\n",
       "  {'name': '洋垃圾', 'value': 0.18072583730500644},\n",
       "  {'name': '超话', 'value': 0.04908631579908009},\n",
       "  {'name': '垃圾桶', 'value': 0.04870221337287133},\n",
       "  {'name': '哥哥', 'value': 0.03379394192200975},\n",
       "  {'name': '回收', 'value': 0.025470424038297725},\n",
       "  {'name': '跟着', 'value': 0.024755504905345397},\n",
       "  {'name': '环境', 'value': 0.023755389264296333},\n",
       "  {'name': '小心', 'value': 0.02371932582171862}],\n",
       " 'sentimentPie': [{'name': '积极', 'value': 0.6371202663337495},\n",
       "  {'name': '消极', 'value': 0.36287973366625054}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppp.SearchNAnalysis('垃圾分类')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
